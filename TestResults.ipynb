{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mamatay ka na sana thank you lord\n",
      "Negative Statement\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "import urllib.request, json\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import joblib\n",
    "from tg_stem import tg_stemmer\n",
    "from eng_tag_lemmatizer import lemmatize_tagalog_english\n",
    "# Download Filipino stopwords\n",
    "with urllib.request.urlopen(\"https://raw.githubusercontent.com/stopwords-iso/stopwords-tl/master/stopwords-tl.json\") as url:\n",
    "    stopwords_tl = json.loads(url.read().decode())\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Remove unwanted characters and digits\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    #Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Convert tokens to lowercase\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    # # Remove stopwords\n",
    "    tokens = [word for word in tokens if word.lower() not in stopwords_tl]\n",
    "    # Lemmatize the tokens\n",
    "    \n",
    "    text = lemmatize_tagalog_english(tokens)\n",
    "\n",
    "    return text\n",
    "\n",
    "    # lemmatized_tokens = []\n",
    "\n",
    "    # for token in tokens:\n",
    "    #     if wordnet.synsets(token):\n",
    "    #         pos = nltk.pos_tag([token])[0][1][0].lower()\n",
    "    #         pos = {'a': wordnet.ADJ,\n",
    "    #                'n': wordnet.NOUN,\n",
    "    #                'v': wordnet.VERB,\n",
    "    #                'r': wordnet.ADV}.get(pos, wordnet.NOUN)\n",
    "    #         lemmatized_token = lemmatizer.lemmatize(token, pos)\n",
    "    #     else:\n",
    "    #         lemmatized_token = tg_stemmer([token])[0]\n",
    "        \n",
    "    #     lemmatized_tokens.append(lemmatized_token)\n",
    "        \n",
    "    # # Join the tokens back into a string\n",
    "    # text = ' '.join(lemmatized_tokens)\n",
    "    # return text\n",
    "\n",
    "\n",
    "# Load the trained SVM model\n",
    "clf = joblib.load('svm_model.joblib')\n",
    "\n",
    "# Load the vectorizer fitted on the training data\n",
    "vectorizer = joblib.load('tfidf_vectorizer.joblib')\n",
    "\n",
    "# Get input from user\n",
    "sentiment = input(\"Enter a sentence to analyze: \")\n",
    "\n",
    "# Preprocess the input text\n",
    "sentiment_processed = preprocess_text(sentiment)\n",
    "\n",
    "# Vectorize the input text\n",
    "sentiment_vectorized = vectorizer.transform([sentiment_processed])\n",
    "\n",
    "# Predict the sentiment using the trained SVM model\n",
    "prediction = clf.predict(sentiment_vectorized)\n",
    "\n",
    "print(sentiment)\n",
    "# Print the prediction\n",
    "if prediction == 1:\n",
    "    print(\"Negative Statement\")\n",
    "else:\n",
    "    print(\"Positive sentiment\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
