{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>clean_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Inaasahan na ni Vice President Jejomar Binay n...</td>\n",
       "      <td>0</td>\n",
       "      <td>inaasahan na ni vice president jejomar binay n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mar Roxas TANG INA TUWID NA DAAN DAW .. EH SYA...</td>\n",
       "      <td>1</td>\n",
       "      <td>mar roxas tang ina tuwid na daan daw    eh sya...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Salamat sa walang sawang suporta ng mga taga m...</td>\n",
       "      <td>0</td>\n",
       "      <td>salamat sa walang sawang suporta ng mga taga m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@rapplerdotcom putangina mo binay TAKBO PA</td>\n",
       "      <td>1</td>\n",
       "      <td>putangina mo binay takbo pa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Binay with selective amnesia, forgetting about...</td>\n",
       "      <td>0</td>\n",
       "      <td>binay with selective amnesia  forgetting about...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4994</th>\n",
       "      <td>@jefk_rew pity those poor people who were invo...</td>\n",
       "      <td>1</td>\n",
       "      <td>pity those poor people who were involved in th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>Awww DUTERTE Na wag Lang si Roxas</td>\n",
       "      <td>0</td>\n",
       "      <td>awww duterte na wag lang si roxas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>RT @mikkieugenio: If the SC disqualifies Poe a...</td>\n",
       "      <td>0</td>\n",
       "      <td>if the sc disqualifies poe as president  eit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>\"Pag naging presidente si Binay, wala kayong t...</td>\n",
       "      <td>0</td>\n",
       "      <td>pag naging presidente si binay  wala kayong t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>Yan na naman ang walang kwentang commercial ni...</td>\n",
       "      <td>1</td>\n",
       "      <td>yan na naman ang walang kwentang commercial ni...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4756 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  label   \n",
       "0     Inaasahan na ni Vice President Jejomar Binay n...      0  \\\n",
       "1     Mar Roxas TANG INA TUWID NA DAAN DAW .. EH SYA...      1   \n",
       "2     Salamat sa walang sawang suporta ng mga taga m...      0   \n",
       "3            @rapplerdotcom putangina mo binay TAKBO PA      1   \n",
       "4     Binay with selective amnesia, forgetting about...      0   \n",
       "...                                                 ...    ...   \n",
       "4994  @jefk_rew pity those poor people who were invo...      1   \n",
       "4995                  Awww DUTERTE Na wag Lang si Roxas      0   \n",
       "4996  RT @mikkieugenio: If the SC disqualifies Poe a...      0   \n",
       "4997  \"Pag naging presidente si Binay, wala kayong t...      0   \n",
       "4998  Yan na naman ang walang kwentang commercial ni...      1   \n",
       "\n",
       "                                            clean_tweet  \n",
       "0     inaasahan na ni vice president jejomar binay n...  \n",
       "1     mar roxas tang ina tuwid na daan daw    eh sya...  \n",
       "2     salamat sa walang sawang suporta ng mga taga m...  \n",
       "3                           putangina mo binay takbo pa  \n",
       "4     binay with selective amnesia  forgetting about...  \n",
       "...                                                 ...  \n",
       "4994  pity those poor people who were involved in th...  \n",
       "4995                  awww duterte na wag lang si roxas  \n",
       "4996    if the sc disqualifies poe as president  eit...  \n",
       "4997   pag naging presidente si binay  wala kayong t...  \n",
       "4998  yan na naman ang walang kwentang commercial ni...  \n",
       "\n",
       "[4756 rows x 3 columns]"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import preprocessor as p\n",
    "import nltk\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "import urllib.request, json \n",
    "\n",
    "def clean_tweets(df):\n",
    "    tempArr = []\n",
    "    for line in df:\n",
    "        # send to tweet_processor\n",
    "        tmpL = p.clean(line)\n",
    "        # remove everything except letters and whitespaces\n",
    "        tmpL = re.sub(r'[^\\w\\s]|[\\d]', ' ', tmpL)\n",
    "        # convert to lowercase\n",
    "        tmpL = tmpL.lower()\n",
    "        tempArr.append(tmpL)\n",
    "    return tempArr\n",
    "\n",
    "REPLACE_NO_SPACE = re.compile(\"(\\d)|(\\.)|(\\;)|(\\:)|(\\!)|(\\')|(\\?)|(\\,)|(\\\")|(\\|)|(\\()|(\\))|(\\[)|(\\])|(\\%)|(\\$)|(\\>)|(\\<)|(\\{)|(\\})\")\n",
    "REPLACE_WITH_SPACE = re.compile(\"(<br\\s/><br\\s/?)|(-)|(/)|(:).\")\n",
    "\n",
    "train = pd.read_csv(r\"C:\\Users\\Admin\\OneDrive\\Documents\\Webdev\\hatespeech\\train.csv\", nrows=5000)\n",
    "#train = pd.read_csv(r\"C:\\Users\\jmest\\Documents\\Files\\C4S2\\Thesis\\ThesisFiles-main\\ThesisFiles-main\\hatespeech\\train.csv\", nrows=5000)\n",
    "\n",
    "\n",
    "train_tweet = clean_tweets(train[\"text\"])\n",
    "train_tweet = pd.DataFrame(train_tweet)\n",
    "train[\"clean_tweet\"] = train_tweet\n",
    "sum(train['clean_tweet'] == '')\n",
    "train['clean_tweet'] = train['clean_tweet'].replace('', np.NaN)\n",
    "train.dropna(axis='rows')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['akin', 'aking', 'ako', 'alin', 'am', 'amin', 'aming', 'ang', 'ano', 'anumang', 'apat', 'at', 'atin', 'ating', 'ay', 'bababa', 'bago', 'bakit', 'bawat', 'bilang', 'dahil', 'dalawa', 'dapat', 'din', 'dito', 'doon', 'gagawin', 'gayunman', 'ginagawa', 'ginawa', 'ginawang', 'gumawa', 'gusto', 'habang', 'hanggang', 'hindi', 'huwag', 'iba', 'ibaba', 'ibabaw', 'ibig', 'ikaw', 'ilagay', 'ilalim', 'ilan', 'inyong', 'isa', 'isang', 'itaas', 'ito', 'iyo', 'iyon', 'iyong', 'ka', 'kahit', 'kailangan', 'kailanman', 'kami', 'kanila', 'kanilang', 'kanino', 'kanya', 'kanyang', 'kapag', 'kapwa', 'karamihan', 'katiyakan', 'katulad', 'kaya', 'kaysa', 'ko', 'kong', 'kulang', 'kumuha', 'kung', 'laban', 'lahat', 'lamang', 'likod', 'lima', 'maaari', 'maaaring', 'maging', 'mahusay', 'makita', 'marami', 'marapat', 'masyado', 'may', 'mayroon', 'mga', 'minsan', 'mismo', 'mula', 'muli', 'na', 'nabanggit', 'naging', 'nagkaroon', 'nais', 'nakita', 'namin', 'napaka', 'narito', 'nasaan', 'ng', 'ngayon', 'ni', 'nila', 'nilang', 'nito', 'niya', 'niyang', 'noon', 'o', 'pa', 'paano', 'pababa', 'paggawa', 'pagitan', 'pagkakaroon', 'pagkatapos', 'palabas', 'pamamagitan', 'panahon', 'pangalawa', 'para', 'paraan', 'pareho', 'pataas', 'pero', 'pumunta', 'pumupunta', 'sa', 'saan', 'sabi', 'sabihin', 'sarili', 'sila', 'sino', 'siya', 'tatlo', 'tayo', 'tulad', 'tungkol', 'una', 'walang']\n"
     ]
    }
   ],
   "source": [
    "train['clean_tweet'] = train['clean_tweet'].replace('', float('NaN'), regex = True)\n",
    "train.dropna(inplace= True)\n",
    "train = train.reset_index(drop=True)\n",
    "first_column = train.pop('label')\n",
    "train.insert(0,'label',first_column)\n",
    "train.dropna(axis='rows')\n",
    "\n",
    "with urllib.request.urlopen(\"https://raw.githubusercontent.com/stopwords-iso/stopwords-tl/master/stopwords-tl.json\") as url:\n",
    "    stopwords = json.loads(url.read().decode())\n",
    "    print(stopwords)\n",
    "\n",
    "\n",
    "train['rm_stpwrds'] = train['clean_tweet'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwords) and not word.isdigit()]))\n",
    "train['tokenize'] = train['rm_stpwrds'].apply(nltk.tokenize.WhitespaceTokenizer().tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>clean_tweet</th>\n",
       "      <th>rm_stpwrds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Inaasahan na ni Vice President Jejomar Binay n...</td>\n",
       "      <td>inaasahan na ni vice president jejomar binay n...</td>\n",
       "      <td>inaasahan vice president jejomar binay taong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mar Roxas TANG INA TUWID NA DAAN DAW .. EH SYA...</td>\n",
       "      <td>mar roxas tang ina tuwid na daan daw    eh sya...</td>\n",
       "      <td>mar roxas tang ina tuwid daan daw eh sya nga d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Salamat sa walang sawang suporta ng mga taga m...</td>\n",
       "      <td>salamat sa walang sawang suporta ng mga taga m...</td>\n",
       "      <td>salamat sawang suporta taga makati pagbabalik ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@rapplerdotcom putangina mo binay TAKBO PA</td>\n",
       "      <td>putangina mo binay takbo pa</td>\n",
       "      <td>putangina mo binay takbo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Binay with selective amnesia, forgetting about...</td>\n",
       "      <td>binay with selective amnesia  forgetting about...</td>\n",
       "      <td>binay with selective amnesia forgetting about ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>It doesn't matter whoever won between Duterte ...</td>\n",
       "      <td>it doesn t matter whoever won between duterte ...</td>\n",
       "      <td>it doesn t matter whoever won between duterte ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Nognog? Pero nognog din ang nag malasakit? Wtf...</td>\n",
       "      <td>nognog  pero nognog din ang nag malasakit  wtf...</td>\n",
       "      <td>nognog nognog nag malasakit wtf tangina mo bin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>#OnlyB1nay ?? #FB https://t.co/QEQnsK67Gm</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>What Abi Binay said on running for Makati mayo...</td>\n",
       "      <td>what abi binay said on running for makati mayor</td>\n",
       "      <td>what abi binay said on running for makati mayor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Srsly. How can Binay do away with no tax for t...</td>\n",
       "      <td>srsly  how can binay do away with no tax for t...</td>\n",
       "      <td>srsly how can binay do away with no tax for th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text   \n",
       "0  Inaasahan na ni Vice President Jejomar Binay n...  \\\n",
       "1  Mar Roxas TANG INA TUWID NA DAAN DAW .. EH SYA...   \n",
       "2  Salamat sa walang sawang suporta ng mga taga m...   \n",
       "3         @rapplerdotcom putangina mo binay TAKBO PA   \n",
       "4  Binay with selective amnesia, forgetting about...   \n",
       "5  It doesn't matter whoever won between Duterte ...   \n",
       "6  Nognog? Pero nognog din ang nag malasakit? Wtf...   \n",
       "7          #OnlyB1nay ?? #FB https://t.co/QEQnsK67Gm   \n",
       "8  What Abi Binay said on running for Makati mayo...   \n",
       "9  Srsly. How can Binay do away with no tax for t...   \n",
       "\n",
       "                                         clean_tweet   \n",
       "0  inaasahan na ni vice president jejomar binay n...  \\\n",
       "1  mar roxas tang ina tuwid na daan daw    eh sya...   \n",
       "2  salamat sa walang sawang suporta ng mga taga m...   \n",
       "3                        putangina mo binay takbo pa   \n",
       "4  binay with selective amnesia  forgetting about...   \n",
       "5  it doesn t matter whoever won between duterte ...   \n",
       "6  nognog  pero nognog din ang nag malasakit  wtf...   \n",
       "7                                                      \n",
       "8    what abi binay said on running for makati mayor   \n",
       "9  srsly  how can binay do away with no tax for t...   \n",
       "\n",
       "                                          rm_stpwrds  \n",
       "0       inaasahan vice president jejomar binay taong  \n",
       "1  mar roxas tang ina tuwid daan daw eh sya nga d...  \n",
       "2  salamat sawang suporta taga makati pagbabalik ...  \n",
       "3                           putangina mo binay takbo  \n",
       "4  binay with selective amnesia forgetting about ...  \n",
       "5  it doesn t matter whoever won between duterte ...  \n",
       "6  nognog nognog nag malasakit wtf tangina mo bin...  \n",
       "7                                                     \n",
       "8    what abi binay said on running for makati mayor  \n",
       "9  srsly how can binay do away with no tax for th...  "
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[['text','clean_tweet','rm_stpwrds']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4756, 5)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOWELS = \"aeiouAEIOU\"\n",
    "CONSONANTS = \"bcdfghklmnngpqrstvwyBCDFGHKLMNNGPQRSTVWY\"\n",
    "\n",
    "\"\"\" \n",
    "\tAffixes\n",
    "\"\"\"\n",
    "PREFIX_SET = [\n",
    "\t'nakikipag', 'pakikipag',\n",
    "\t'pinakama', 'pagpapa',\n",
    "\t'pinagka', 'panganga', \n",
    "\t'makapag', 'nakapag', \n",
    "\t'tagapag', 'makipag', \n",
    "\t'nakipag', 'tigapag',\n",
    "\t'pakiki', 'magpa',\n",
    "\t'napaka', 'pinaka',\n",
    "\t'ipinag', 'pagka', \n",
    "\t'pinag', 'mapag', \n",
    "\t'mapa', 'taga', \n",
    "\t'ipag', 'tiga', \n",
    "\t'pala', 'pina', \n",
    "\t'pang', 'naka',\n",
    "\t'nang', 'mang',\n",
    "\t'sing',\n",
    "\t'ipa', 'pam',\n",
    "\t'pan', 'pag',\n",
    "\t'tag', 'mai',\n",
    "\t'mag', 'nam',\n",
    "\t'nag', 'man',\n",
    "\t'may', 'ma',\n",
    "\t'na', 'ni',\n",
    "\t'pa', 'ka',\n",
    "\t'um', 'in',\n",
    "\t'i',\n",
    "]\n",
    "\n",
    "INFIX_SET = [\n",
    "\t'um', 'in',\n",
    "]\n",
    "\n",
    "SUFFIX_SET = [\n",
    "\t'syon','dor', \n",
    "\t'ita', 'han', \n",
    "\t'hin', 'ing', \n",
    "\t'ang', 'ng', \n",
    "\t'an', 'in', \n",
    "\t'g',\n",
    "]\n",
    "\n",
    "PERIOD_FLAG = True\n",
    "PASS_FLAG = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def check_vowel(substring):\n",
    "\t\"\"\"\n",
    "\t\tChecks if the substring is a vowel.\n",
    "\t\t\tletters: substring to be tested\n",
    "\t\treturns BOOLEAN\n",
    "\t\"\"\"\n",
    "\n",
    "\treturn all(letter in VOWELS for letter in substring)\n",
    "\n",
    "\n",
    "def check_consonant(substring):\n",
    "\t\"\"\"\n",
    "\t\tChecks if the letter is a consonant.\n",
    "\t\t\tletter: substring to be tested\n",
    "\t\treturns BOOLEAN\n",
    "\t\"\"\"\n",
    "\n",
    "\treturn all(letter in CONSONANTS for letter in substring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_letter(token, index, letter):\n",
    "\t\"\"\"\n",
    "\t\tReplaces a letter in a token.\n",
    "\t\t\ttoken: word to be used\n",
    "\t\t\tindex: index of the letter\n",
    "\t\t\tletter: letter used to replace\n",
    "\t\treturns STRING\n",
    "\t\"\"\"\n",
    "\t\n",
    "\t_list = list(token)\n",
    "\t_list[index] = letter\n",
    "\n",
    "\treturn ''.join(_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_vowel(token):\n",
    "\t\"\"\"\n",
    "\t\tCount vowels in a given token.\n",
    "\t\t\ttoken: string to be counted for vowels\n",
    "\t\treturns INTEGER\n",
    "\t\"\"\"\n",
    "\n",
    "\tcount = 0\n",
    "\n",
    "\tfor tok in token:\n",
    "\t\tif check_vowel(tok):\n",
    "\t\t\tcount+=1\n",
    "\n",
    "\treturn count\n",
    "\n",
    "\n",
    "def count_consonant(token):\n",
    "\t\"\"\"\n",
    "\t\tCount consonants in a given token.\n",
    "\t\t\ttoken: string to be counted for consonants\n",
    "\t\treturns INTEGER\n",
    "\t\"\"\"\n",
    "\n",
    "\tcount = 0\n",
    "\n",
    "\tfor tok in token:\n",
    "\t\tif check_consonant(tok):\n",
    "\t\t\tcount+=1\n",
    "\n",
    "\treturn count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_validation(token):\n",
    "    with open('validation.txt', 'r') as valid:\n",
    "        data = valid.read().replace('\\n', ' ').split(' ')\n",
    "\n",
    "    return token in data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_repitition(token, REPITITION):\n",
    "\t\"\"\"\n",
    "\t\tChecks token for repitition. (ex. nakakabaliw = nabaliw)\n",
    "\t\t\ttoken: word to be stemmed repitition\n",
    "\t\treturns STRING\n",
    "\t\"\"\"\n",
    "\n",
    "\tif check_validation(token):\n",
    "\t\treturn token\n",
    "\n",
    "\tif len(token) >= 4:\n",
    "\t\tif check_vowel(token[0]):\n",
    "\t\t\tif token[0] == token[1]:\n",
    "\t\t\t\tREPITITION.append(token[0])\n",
    "\t\t\t\treturn token[1:]\n",
    "\n",
    "\t\telif check_consonant(token[0]) and count_vowel(token) >= 2:\n",
    "\t\t\tif token[0: 2] == token[2: 4] and len(token) - 2 >= 4:\n",
    "\t\t\t\tREPITITION.append(token[2:4])\n",
    "\t\t\t\treturn token[2:]\n",
    "\t\t\t\n",
    "\t\t\telif token[0: 3] == token[3: 6] and len(token) - 3 >= 4:\n",
    "\t\t\t\tREPITITION.append(token[3:6])\n",
    "\t\t\t\treturn token[3:]\n",
    "\n",
    "\treturn token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_suffix(token, SUFFIX):\n",
    "    \"\"\"\n",
    "    Checks token for suffixes. (ex. bigayan = bigay)\n",
    "        token: word to be stemmed for suffixes\n",
    "    returns STRING\n",
    "    \"\"\"\n",
    "\n",
    "    SUF_CANDIDATE = []\n",
    "\n",
    "    if check_validation(token):\n",
    "        return token\n",
    "\n",
    "    for suffix in SUFFIX_SET:\n",
    "        if len(token) - len(suffix) >= 3 and count_vowel(token[0:len(token) - len(suffix)]) >= 2 and count_consonant(token[0:len(token) - len(suffix)]) >= 1:\n",
    "            if token[len(token) - len(suffix): len(token)] == suffix:\n",
    "                if len(suffix) == 2 and not count_consonant(token[0:len(token) - len(suffix)]) >= 1:\n",
    "                    continue\n",
    "\n",
    "                if count_vowel(token[0: len(token) - len(suffix)]) >= 2:\n",
    "                    if suffix == 'ang' and check_consonant(token[-4]) \\\n",
    "                            and token[-4] != 'r' and token[-5] != 'u':\n",
    "                        continue\n",
    "\n",
    "                    print(token[0: len(token) - len(suffix)] + \" : \" + suffix)\n",
    "\n",
    "                    if check_validation(token[0: len(token) - len(suffix)]):\n",
    "                        SUFFIX.append(suffix)\n",
    "                        return token[0: len(token) - len(suffix)] + 'a' if suffix == 'ita' \\\n",
    "                            else token[0: len(token) - len(suffix)]\n",
    "\n",
    "                    elif len(SUF_CANDIDATE) == 0:\n",
    "                        SUF_CANDIDATE.append(suffix)\n",
    "                        SUF_CANDIDATE.append(token[0: len(token) - len(suffix)])\n",
    "\n",
    "    if (len(SUF_CANDIDATE) == 2):\n",
    "        SUFFIX = SUF_CANDIDATE[0]\n",
    "        return SUF_CANDIDATE[1][0: len(token) - len(SUFFIX)] + 'a' if SUFFIX == 'ita' \\\n",
    "            else SUF_CANDIDATE[1][0: len(token) - len(SUFFIX)]\n",
    "\n",
    "    return token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_infix(token, INFIX):\n",
    "\t\"\"\"\n",
    "\t\tChecks token for infixes. (ex. bumalik = balik)\n",
    "\t\t\ttoken: word to be stemmed for infixes\n",
    "\t\treturns STRING\n",
    "\t\"\"\"\n",
    "\n",
    "\tif check_validation(token):\n",
    "\t\treturn token\n",
    "\n",
    "\tfor infix in INFIX_SET:\n",
    "\t\tif len(token) - len(infix) >= 3 and count_vowel(token[len(infix):]) >= 2:\n",
    "\t\t\tif token[0] == token[4] and token[1: 4] == infix:\n",
    "\t\t\t\tINFIX.append(infix)\n",
    "\t\t\t\treturn token[4:]\n",
    "\n",
    "\t\t\telif token[2] == token[4] and token[1: 3] == infix:\n",
    "\t\t\t\tINFIX.append(infix)\n",
    "\t\t\t\treturn token[0] + token[3:]\n",
    "\n",
    "\t\t\telif token[1: 3] == infix and check_vowel(token[3]):\n",
    "\t\t\t\tINFIX.append(infix)\n",
    "\t\t\t\treturn token[0] + token[3:]\n",
    "\n",
    "\treturn token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_prefix(token,\t PREFIX):\n",
    "\t\"\"\"\n",
    "\t\tChecks token for prefixes. (ex. naligo = ligo)\n",
    "\t\t\ttoken: word to be stemmed for prefixes\n",
    "\t\treturns STRING\n",
    "\t\"\"\"\n",
    "\n",
    "\tif check_validation(token):\n",
    "\t\treturn token\n",
    "\n",
    "\tfor prefix in PREFIX_SET:\n",
    "\t\tif len(token) - len(prefix) >= 3 and \\\n",
    "\t\t\tcount_vowel(token[len(prefix):]) >= 2:\n",
    "\n",
    "\t\t\tif prefix == ('i') and check_consonant(token[2]):\n",
    "\t\t\t\tcontinue\n",
    "\n",
    "\t\t\tif '-' in token:\t\n",
    "\t\t\t\ttoken = token.split('-')\n",
    "\n",
    "\t\t\t\tif token[0] == prefix and check_vowel(token[1][0]):\n",
    "\t\t\t\t\tPREFIX.append(prefix)\n",
    "\t\t\t\t\treturn token[1]\n",
    "\n",
    "\t\t\t\ttoken = '-'.join(token)\n",
    "\n",
    "\t\t\tif token[0: len(prefix)] == prefix:\n",
    "\t\t\t\tif count_vowel(token[len(prefix):]) >= 2:\n",
    "\t\t\t\t\t# if check_vowel(token[len(token) - len(prefix) - 1]):\n",
    "\t\t\t\t# \tcontinue\n",
    "\n",
    "\t\t\t\t\tif prefix == 'panganga':\n",
    "\t\t\t\t\t\tPREFIX.append(prefix)\n",
    "\t\t\t\t\t\treturn 'ka' + token[len(prefix):]\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\tPREFIX.append(prefix)\n",
    "\t\t\t\t\treturn token[len(prefix):]\n",
    "\n",
    "\treturn token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_duplication(token, DUPLICATE):\n",
    "\t\"\"\"\n",
    "\t\tChecks token for duplication. (ex. araw-araw = araw)\n",
    "\t\t\ttoken: word to be stemmed duplication\n",
    "\t\treturns STRING\n",
    "\t\"\"\"\n",
    "\n",
    "\tif check_validation(token):\n",
    "\t\treturn token\n",
    "\n",
    "\tif '-' in token and token.index('-') != 0 and \\\n",
    "\t\ttoken.index('-') != len(token) -  1:\n",
    "\n",
    "\t\tsplit = token.split('-')\n",
    "\n",
    "\t\tif all(len(tok) >= 3 for tok in split):\n",
    "\t\t\tif split[0] == token[1] or split[0][-1] == 'u' and change_letter(split[0], -1, 'o') == split[1] or \\\n",
    "\t\t\t\tsplit[0][-2] == 'u' and change_letter(split[0], -2, 'o')  == split[1]:\n",
    "\t\t\t\tDUPLICATE.append(split[0])\n",
    "\t\t\t\treturn split[0]\n",
    "\n",
    "\t\t\telif split[0] == split[1][0:len(split[0])]:\n",
    "\t\t\t\tDUPLICATE.append(split[1])\n",
    "\t\t\t\treturn split[1]\n",
    "\n",
    "\t\t\telif split[0][-2:] == 'ng':\n",
    "\t\t\t\tif split[0][-3] == 'u':\n",
    "\t\t\t\t\tif split[0][0:-3] + 'o' == split[1]:\n",
    "\t\t\t\t\t\tDUPLICATE.append(split[1])\n",
    "\t\t\t\t\t\treturn split[1]\n",
    "\n",
    "\t\t\t\tif split[0][0:-2] == split[1]:\n",
    "\t\t\t\t\tDUPLICATE.append(split[1])\n",
    "\t\t\t\t\treturn split[1]\n",
    "\n",
    "\t\telse:\n",
    "\t\t\treturn '-'.join(split)\n",
    "\t\n",
    "\treturn token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_repitition(token, REPITITION):\n",
    "\t\"\"\"\n",
    "\t\tChecks token for repitition. (ex. nakakabaliw = nabaliw)\n",
    "\t\t\ttoken: word to be stemmed repitition\n",
    "\t\treturns STRING\n",
    "\t\"\"\"\n",
    "\n",
    "\tif check_validation(token):\n",
    "\t\treturn token\n",
    "\n",
    "\tif len(token) >= 4:\n",
    "\t\tif check_vowel(token[0]):\n",
    "\t\t\tif token[0] == token[1]:\n",
    "\t\t\t\tREPITITION.append(token[0])\n",
    "\t\t\t\treturn token[1:]\n",
    "\n",
    "\t\telif check_consonant(token[0]) and count_vowel(token) >= 2:\n",
    "\t\t\tif token[0: 2] == token[2: 4] and len(token) - 2 >= 4:\n",
    "\t\t\t\tREPITITION.append(token[2:4])\n",
    "\t\t\t\treturn token[2:]\n",
    "\t\t\t\n",
    "\t\t\telif token[0: 3] == token[3: 6] and len(token) - 3 >= 4:\n",
    "\t\t\t\tREPITITION.append(token[3:6])\n",
    "\t\t\t\treturn token[3:]\n",
    "\n",
    "\treturn token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_stemmed(token, CLEANERS, REPITITION):\n",
    "\t\"\"\"\n",
    "\t\tChecks for left-over affixes and letters.\n",
    "\t\t\ttoken: word to be cleaned for excess affixes/letters\n",
    "\t\treturns STRING\n",
    "\t\"\"\"\n",
    "\n",
    "\tglobal PERIOD_FLAG\n",
    "\tglobal PASS_FLAG\n",
    "\n",
    "\tCC_EXP = ['dr', 'gl', 'gr', 'ng', 'kr', 'kl', 'kw', 'ts', 'tr', 'pr', 'pl', 'pw', 'sw', 'sy'] # Consonant + Consonant Exceptions\n",
    "\n",
    "\tif token[-1] == '.' and PASS_FLAG == False:\n",
    "\t\tPERIOD_FLAG = True\n",
    "\n",
    "\tif not check_vowel(token[-1]) and not check_consonant(token[-1]):\n",
    "\t\tCLEANERS.append(token[-1])\n",
    "\t\ttoken = token[0:-1]\n",
    "\n",
    "\tif not check_vowel(token[0]) and not check_consonant(token[0]):\n",
    "\t\tCLEANERS.append(token[0])\n",
    "\t\ttoken = token[1:]\n",
    "\n",
    "\tif check_validation(token):\n",
    "\t\treturn token\n",
    "\n",
    "\tif len(token) >= 3 and count_vowel(token) >= 2:\n",
    "\t\ttoken = clean_repitition(token,\tREPITITION)\n",
    "\n",
    "\t\tif check_consonant(token[-1]) and token[- 2] == 'u':\n",
    "\t\t\tCLEANERS.append('u')\n",
    "\t\t\ttoken = change_letter(token, -2, 'o')\n",
    "\n",
    "\t\tif token[len(token) - 1] == 'u':\n",
    "\t\t\tCLEANERS.append('u')\n",
    "\t\t\ttoken = change_letter(token, -1, 'o')\n",
    "\n",
    "\t\tif token[-1] == 'r':\n",
    "\t\t\tCLEANERS.append('r')\n",
    "\t\t\ttoken = change_letter(token, -1, 'd')\n",
    "\n",
    "\t\tif token[-1] == 'h' and check_vowel(token[-1]):\n",
    "\t\t\tCLEANERS.append('h')\n",
    "\t\t\ttoken = token[0:-1]\n",
    "\n",
    "\t\t# if token[0] == 'i':\n",
    "\t\t# \ttoken = token[1:]\n",
    "\n",
    "\t\tif token[0] == token[1]:\n",
    "\t\t\tCLEANERS.append(token[0])\n",
    "\t\t\ttoken = token[1:]\n",
    "\n",
    "\t\tif (token[0: 2] == 'ka' or token[0: 2] == 'pa') and check_consonant(token[2]) \\\n",
    "\t\t\tand count_vowel(token) >= 3:\n",
    "\t\t\t\n",
    "\t\t\tCLEANERS.append(token[0: 2])\n",
    "\t\t\ttoken = token[2:]\n",
    "\n",
    "\t\tif(token[-3:]) == 'han' and count_vowel(token[0:-3]) == 1:\n",
    "\t\t\tCLEANERS.append('han')\n",
    "\t\t\ttoken = token[0:-3] + 'i'\n",
    "\n",
    "\t\tif(token[-3:]) == 'han' and count_vowel(token[0:-3]) > 1:\n",
    "\t\t\tCLEANERS.append('han')\n",
    "\t\t\ttoken = token[0:-3]\n",
    "\n",
    "\t\tif len(token) >= 2 and count_vowel(token) >= 3:\n",
    "\t\t\tif token[-1] == 'h' and check_vowel(token[-2]):\n",
    "\t\t\t\tCLEANERS.append('h')\n",
    "\t\t\t\ttoken = token[0:-1]\n",
    "\n",
    "\t\tif len(token) >= 6 and token[0:2] == token[2:4]:\n",
    "\t\t\tCLEANERS.append('0:2')\n",
    "\t\t\ttoken = token[2:]\n",
    "\n",
    "\t\tif any(REP[0] == 'r' for REP in REPITITION):\n",
    "\t\t\tCLEANERS.append('r')\n",
    "\t\t\ttoken = change_letter(token, 0, 'd')\n",
    "\n",
    "\t\tif token[-2:] == 'ng' and token[-3] == 'u':\n",
    "\t\t\tCLEANERS.append('u')\n",
    "\t\t\ttoken = change_letter(token, -3, 'o')\n",
    "\n",
    "\t\tif token[-1] == 'h':\n",
    "\t\t\tCLEANERS.append('h')\n",
    "\t\t\ttoken = token[0:-1]\n",
    "\n",
    "\t\tif any(token[0:2] != CC for CC in CC_EXP) and check_consonant(token[0:2]):\n",
    "\t\t\tCLEANERS.append(token[0:2])\n",
    "\t\t\ttoken = token[1:]\n",
    "\n",
    "\treturn token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemmer(tokens):\n",
    "\n",
    "\tglobal PERIOD_FLAG\n",
    "\tglobal PASS_FLAG\n",
    "\n",
    "\tpre_stem     = inf_stem = suf_stem = rep_stem = \\\n",
    "\t\tdu1_stem = du2_stem = cle_stem = '-'\n",
    "\tword_info    = {}\n",
    "\tPREFIX     = []\n",
    "\tINFIX      = []\n",
    "\tSUFFIX     = []\n",
    "\tDUPLICATE  = []\n",
    "\tREPITITION = []\n",
    "\tCLEANERS   = []\n",
    "\n",
    "\tfor token in tokens:\n",
    "\t\t\t\n",
    "\t\t\tword_info[\"word\"] = token\n",
    "\t\t\t\n",
    "\t\t\tif (PERIOD_FLAG == True and token[0].isupper()) or \\\n",
    "\t\t\t\t(PERIOD_FLAG == False and token[0].islower()):\n",
    "\n",
    "\t\t\t\ttoken \t = token.lower()\t\t\n",
    "\t\t\t\tdu1_stem = clean_duplication(token, DUPLICATE)\n",
    "\t\t\t\tpre_stem = clean_prefix(du1_stem, PREFIX)\n",
    "\t\t\t\trep_stem = clean_repitition(pre_stem, REPITITION)\n",
    "\t\t\t\tinf_stem = clean_infix(rep_stem, INFIX)\n",
    "\t\t\t\trep_stem = clean_repitition(inf_stem, REPITITION)\n",
    "\t\t\t\tsuf_stem = clean_suffix(rep_stem, SUFFIX)\n",
    "\t\t\t\tdu2_stem = clean_duplication(suf_stem, DUPLICATE)\n",
    "\t\t\t\tcle_stem = clean_stemmed(du2_stem, CLEANERS, REPITITION)\n",
    "\t\t\t\tcle_stem = clean_duplication(cle_stem, DUPLICATE)\n",
    "\n",
    "\t\t\telse:\n",
    "\t\t\t\tPERIOD_FLAG = False\n",
    "\t\t\t\tcle_stem = clean_stemmed(token, CLEANERS, REPITITION)\n",
    "\t\t\t\tword_info[\"root\"]   = token\n",
    "\t\t\t\tword_info[\"prefix\"] = '[]'\n",
    "\t\t\t\tword_info[\"infix\"]  = '[]'\n",
    "\t\t\t\tword_info[\"suffix\"] = '[]'\n",
    "\t\t\t\tword_info[\"repeat\"] = '[]'\n",
    "\t\t\t\tword_info[\"dupli\"]  = '[]'\n",
    "\t\t\t\tword_info[\"clean\"]   = '[]'\n",
    "\t\t\t\n",
    "\t\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['tokenize'].to_csv('tokenize.txt', sep=' ', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tao : ng\n",
      "sawa : ng\n",
      "forgett : ing\n",
      "forgetti : ng\n",
      "forgettin : g\n",
      "prepar : ing\n",
      "prepari : ng\n",
      "preparin : g\n",
      "nogno : g\n",
      "nogno : g\n",
      "runni : ng\n",
      "runnin : g\n",
      "earn : ing\n",
      "earni : ng\n",
      "earnin : g\n",
      "hirap : an\n",
      "endi : ng\n",
      "endin : g\n",
      "aga : in\n",
      "nogno : g\n",
      "break : ing\n",
      "breaki : ng\n",
      "breakin : g\n",
      "ito : ng\n",
      "niyo : ng\n",
      "batikus : in\n",
      "talaga : ng\n",
      "para : ng\n",
      "gusto : ng\n",
      "hirapa : ng\n",
      "para : ng\n",
      "kipa : g\n",
      "talun : in\n",
      "bei : ng\n",
      "bein : g\n",
      "para : ng\n",
      "uili : an\n",
      "kair : ita\n",
      "goberna : dor\n",
      "salamat : an\n",
      "nakaw : an\n",
      "handa : ng\n",
      "ibago : ng\n",
      "nogno : g\n",
      "stumpi : ng\n",
      "stumpin : g\n",
      "talaga : ng\n",
      "calli : ng\n",
      "callin : g\n",
      "bayar : an\n",
      "tanung : in\n",
      "inta : ng\n",
      "intan : g\n",
      "basa : hin\n",
      "endors : ing\n",
      "endorsi : ng\n",
      "endorsin : g\n",
      "tulung : an\n",
      "ombudsm : an\n",
      "kelang : an\n",
      "itim : in\n",
      "tuwa : ng\n",
      "tau : ng\n",
      "taun : g\n",
      "annoy : ing\n",
      "annoyi : ng\n",
      "annoyin : g\n",
      "ano : ng\n",
      "andam : ing\n",
      "andami : ng\n",
      "syado : ng\n",
      "syadon : g\n",
      "sobra : ng\n",
      "givi : ng\n",
      "givin : g\n",
      "earn : ing\n",
      "earni : ng\n",
      "earnin : g\n",
      "syado : ng\n",
      "syadon : g\n",
      "halata : ng\n",
      "daa : ng\n",
      "daan : g\n",
      "risi : ng\n",
      "risin : g\n",
      "kair : ita\n",
      "nothi : ng\n",
      "nothin : g\n",
      "gawi : ng\n",
      "igi : ng\n",
      "suporta : han\n",
      "kilalan : in\n",
      "sabi : ng\n",
      "baytular : an\n",
      "upris : ing\n",
      "uprisi : ng\n",
      "uprisin : g\n",
      "ako : ng\n",
      "muda : ng\n",
      "mudan : g\n",
      "kayo : ng\n",
      "daa : ng\n",
      "daan : g\n",
      "nala : ng\n",
      "nalan : g\n",
      "nala : ng\n",
      "nalan : g\n",
      "aga : in\n",
      "para : ng\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "string index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[301], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Perform stemming\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m train[\u001b[39m'\u001b[39m\u001b[39mstemmed\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m train[\u001b[39m'\u001b[39;49m\u001b[39mtokenize\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mapply(stemmer)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\series.py:4631\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4521\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[0;32m   4522\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   4523\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4526\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   4527\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[0;32m   4528\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   4529\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4530\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4629\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4630\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4631\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\u001b[39mself\u001b[39;49m, func, convert_dtype, args, kwargs)\u001b[39m.\u001b[39;49mapply()\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\apply.py:1025\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_str()\n\u001b[0;32m   1024\u001b[0m \u001b[39m# self.f is Callable\u001b[39;00m\n\u001b[1;32m-> 1025\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\apply.py:1076\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1074\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1075\u001b[0m         values \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m)\u001b[39m.\u001b[39m_values\n\u001b[1;32m-> 1076\u001b[0m         mapped \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49mmap_infer(\n\u001b[0;32m   1077\u001b[0m             values,\n\u001b[0;32m   1078\u001b[0m             f,\n\u001b[0;32m   1079\u001b[0m             convert\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_dtype,\n\u001b[0;32m   1080\u001b[0m         )\n\u001b[0;32m   1082\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1083\u001b[0m     \u001b[39m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1084\u001b[0m     \u001b[39m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1085\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39m_constructor_expanddim(\u001b[39mlist\u001b[39m(mapped), index\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\_libs\\lib.pyx:2834\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[299], line 36\u001b[0m, in \u001b[0;36mstemmer\u001b[1;34m(tokens)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     35\u001b[0m \tPERIOD_FLAG \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m \tcle_stem \u001b[39m=\u001b[39m clean_stemmed(token, CLEANERS, REPITITION)\n\u001b[0;32m     37\u001b[0m \tword_info[\u001b[39m\"\u001b[39m\u001b[39mroot\u001b[39m\u001b[39m\"\u001b[39m]   \u001b[39m=\u001b[39m token\n\u001b[0;32m     38\u001b[0m \tword_info[\u001b[39m\"\u001b[39m\u001b[39mprefix\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m[]\u001b[39m\u001b[39m'\u001b[39m\n",
      "Cell \u001b[1;32mIn[298], line 20\u001b[0m, in \u001b[0;36mclean_stemmed\u001b[1;34m(token, CLEANERS, REPITITION)\u001b[0m\n\u001b[0;32m     17\u001b[0m \tCLEANERS\u001b[39m.\u001b[39mappend(token[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[0;32m     18\u001b[0m \ttoken \u001b[39m=\u001b[39m token[\u001b[39m0\u001b[39m:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m---> 20\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m check_vowel(token[\u001b[39m0\u001b[39;49m]) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m check_consonant(token[\u001b[39m0\u001b[39m]):\n\u001b[0;32m     21\u001b[0m \tCLEANERS\u001b[39m.\u001b[39mappend(token[\u001b[39m0\u001b[39m])\n\u001b[0;32m     22\u001b[0m \ttoken \u001b[39m=\u001b[39m token[\u001b[39m1\u001b[39m:]\n",
      "\u001b[1;31mIndexError\u001b[0m: string index out of range"
     ]
    }
   ],
   "source": [
    "# Perform stemming\n",
    "train['stemmed'] = train['tokenize'].apply(stemmer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
