{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import neccessary dependencies\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import svm\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import joblib\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data\n",
    "# train = pd.read_csv(r\"C:\\Users\\Admin\\OneDrive\\Documents\\Webdev\\hatespeech\\train.csv\", nrows=5000)\n",
    "\n",
    "# testing data\n",
    "# test = pd.read_csv(r\"C:\\Users\\Admin\\OneDrive\\Documents\\Webdev\\hatespeech\\test.csv\", nrows=5000)\n",
    "\n",
    "\n",
    "# # # #training data\n",
    "train = pd.read_csv(r\"C:\\Users\\jmest\\Documents\\Files\\C4S2\\Thesis\\ThesisFiles-main\\ThesisFiles-main\\hatespeech\\train.csv\")\n",
    "\n",
    "# # # #testing data\n",
    "test = pd.read_csv(r\"C:\\Users\\jmest\\Documents\\Files\\C4S2\\Thesis\\ThesisFiles-main\\ThesisFiles-main\\hatespeech\\test.csv\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean the data of null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Inaasahan na ni Vice President Jejomar Binay n...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mar Roxas TANG INA TUWID NA DAAN DAW .. EH SYA...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Salamat sa walang sawang suporta ng mga taga m...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@rapplerdotcom putangina mo binay TAKBO PA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Binay with selective amnesia, forgetting about...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>Mar Roxas on the rise, w/ momentum, machinery,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>@chelseapailmao</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>Alan Cayetano 'confirms' Palace, Roxas, Poe be...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>Mas nakakainis ad ni Mar kaysa kay Binay.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>Preliminary and partial results coming in sugg...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  label\n",
       "0     Inaasahan na ni Vice President Jejomar Binay n...      0\n",
       "1     Mar Roxas TANG INA TUWID NA DAAN DAW .. EH SYA...      1\n",
       "2     Salamat sa walang sawang suporta ng mga taga m...      0\n",
       "3            @rapplerdotcom putangina mo binay TAKBO PA      1\n",
       "4     Binay with selective amnesia, forgetting about...      0\n",
       "...                                                 ...    ...\n",
       "9995  Mar Roxas on the rise, w/ momentum, machinery,...      0\n",
       "9996                                    @chelseapailmao      1\n",
       "9997  Alan Cayetano 'confirms' Palace, Roxas, Poe be...      0\n",
       "9998          Mas nakakainis ad ni Mar kaysa kay Binay.      1\n",
       "9999  Preliminary and partial results coming in sugg...      0\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop rows with null values\n",
    "train.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text     0\n",
       "label    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check for null values in train\n",
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5340"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for 0 values in train\n",
    "sum(train[\"label\"] == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4660"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for 1 values in train\n",
    "sum(train[\"label\"] == 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean the data of unwated Text and Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove special characters using the regular expression library\n",
    "\n",
    "import re\n",
    "\n",
    "#set up punctuations we want to be replaced\n",
    "\n",
    "REPLACE_NO_SPACE = re.compile(\"(\\.)|(\\;)|(\\:)|(\\!)|(\\')|(\\?)|(\\,)|(\\\")|(\\|)|(\\()|(\\))|(\\[)|(\\])|(\\%)|(\\$)|(\\>)|(\\<)|(\\{)|(\\})\")\n",
    "REPLACE_WITH_SPACE = re.compile(\"(<br\\s/><br\\s/?)|(-)|(/)|(:).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import preprocessor as p\n",
    "\n",
    "# custum function to clean the dataset (combining tweet_preprocessor and reguar expression)\n",
    "def clean_tweets(df):\n",
    "    tempArr = []\n",
    "    for line in df:\n",
    "        # send to tweet_processor\n",
    "        tmpL = p.clean(line)\n",
    "        # remove everything except letters and digits\n",
    "        tmpL = re.sub(r'[^a-zA-Z0-9\\s]', ' ', tmpL)\n",
    "        # convert to lowercase\n",
    "        tmpL = tmpL.lower()\n",
    "        tempArr.append(tmpL)\n",
    "    return tempArr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean training data\n",
    "train_tweet = clean_tweets(train[\"text\"])\n",
    "train_tweet = pd.DataFrame(train_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append cleaned tweets to the training data\n",
    "train[\"clean_tweet\"] = train_tweet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "494"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(train['clean_tweet'] == '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['Another'] = df['Another'].replace('', np.nan)\n",
    "#replace all empty spaces with NaN to drop using dropna\n",
    "train['clean_tweet'] = train['clean_tweet'].replace('', np.NaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>clean_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Inaasahan na ni Vice President Jejomar Binay n...</td>\n",
       "      <td>0</td>\n",
       "      <td>inaasahan na ni vice president jejomar binay n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mar Roxas TANG INA TUWID NA DAAN DAW .. EH SYA...</td>\n",
       "      <td>1</td>\n",
       "      <td>mar roxas tang ina tuwid na daan daw    eh sya...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Salamat sa walang sawang suporta ng mga taga m...</td>\n",
       "      <td>0</td>\n",
       "      <td>salamat sa walang sawang suporta ng mga taga m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@rapplerdotcom putangina mo binay TAKBO PA</td>\n",
       "      <td>1</td>\n",
       "      <td>putangina mo binay takbo pa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Binay with selective amnesia, forgetting about...</td>\n",
       "      <td>0</td>\n",
       "      <td>binay with selective amnesia  forgetting about...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9994</th>\n",
       "      <td>kaninang pa itong Binay binay binay.....tch</td>\n",
       "      <td>1</td>\n",
       "      <td>kaninang pa itong binay binay binay     tch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>Mar Roxas on the rise, w/ momentum, machinery,...</td>\n",
       "      <td>0</td>\n",
       "      <td>mar roxas on the rise  w  momentum  machinery ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>Alan Cayetano 'confirms' Palace, Roxas, Poe be...</td>\n",
       "      <td>0</td>\n",
       "      <td>alan cayetano  confirms  palace  roxas  poe be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>Mas nakakainis ad ni Mar kaysa kay Binay.</td>\n",
       "      <td>1</td>\n",
       "      <td>mas nakakainis ad ni mar kaysa kay binay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>Preliminary and partial results coming in sugg...</td>\n",
       "      <td>0</td>\n",
       "      <td>preliminary and partial results coming in sugg...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9506 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  label  \\\n",
       "0     Inaasahan na ni Vice President Jejomar Binay n...      0   \n",
       "1     Mar Roxas TANG INA TUWID NA DAAN DAW .. EH SYA...      1   \n",
       "2     Salamat sa walang sawang suporta ng mga taga m...      0   \n",
       "3            @rapplerdotcom putangina mo binay TAKBO PA      1   \n",
       "4     Binay with selective amnesia, forgetting about...      0   \n",
       "...                                                 ...    ...   \n",
       "9994        kaninang pa itong Binay binay binay.....tch      1   \n",
       "9995  Mar Roxas on the rise, w/ momentum, machinery,...      0   \n",
       "9997  Alan Cayetano 'confirms' Palace, Roxas, Poe be...      0   \n",
       "9998          Mas nakakainis ad ni Mar kaysa kay Binay.      1   \n",
       "9999  Preliminary and partial results coming in sugg...      0   \n",
       "\n",
       "                                            clean_tweet  \n",
       "0     inaasahan na ni vice president jejomar binay n...  \n",
       "1     mar roxas tang ina tuwid na daan daw    eh sya...  \n",
       "2     salamat sa walang sawang suporta ng mga taga m...  \n",
       "3                           putangina mo binay takbo pa  \n",
       "4     binay with selective amnesia  forgetting about...  \n",
       "...                                                 ...  \n",
       "9994        kaninang pa itong binay binay binay     tch  \n",
       "9995  mar roxas on the rise  w  momentum  machinery ...  \n",
       "9997  alan cayetano  confirms  palace  roxas  poe be...  \n",
       "9998          mas nakakainis ad ni mar kaysa kay binay   \n",
       "9999  preliminary and partial results coming in sugg...  \n",
       "\n",
       "[9506 rows x 3 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.dropna(axis='rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://statisticsglobe.com/drop-rows-blank-values-from-pandas-dataframe-python\n",
    "train['clean_tweet'] = train['clean_tweet'].replace('', float('NaN'), regex = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.dropna(inplace= True)\n",
    "train = train.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_column = train.pop('label')\n",
    "train.insert(0,'label',first_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9506, 3)\n"
     ]
    }
   ],
   "source": [
    "#total data entries for training\n",
    "\n",
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5030"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for 0 values in train\n",
    "sum(train[\"label\"] == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4476"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for 1 values in train\n",
    "sum(train[\"label\"] == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['akin', 'aking', 'ako', 'alin', 'am', 'amin', 'aming', 'ang', 'ano', 'anumang', 'apat', 'at', 'atin', 'ating', 'ay', 'bababa', 'bago', 'bakit', 'bawat', 'bilang', 'dahil', 'dalawa', 'dapat', 'din', 'dito', 'doon', 'gagawin', 'gayunman', 'ginagawa', 'ginawa', 'ginawang', 'gumawa', 'gusto', 'habang', 'hanggang', 'hindi', 'huwag', 'iba', 'ibaba', 'ibabaw', 'ibig', 'ikaw', 'ilagay', 'ilalim', 'ilan', 'inyong', 'isa', 'isang', 'itaas', 'ito', 'iyo', 'iyon', 'iyong', 'ka', 'kahit', 'kailangan', 'kailanman', 'kami', 'kanila', 'kanilang', 'kanino', 'kanya', 'kanyang', 'kapag', 'kapwa', 'karamihan', 'katiyakan', 'katulad', 'kaya', 'kaysa', 'ko', 'kong', 'kulang', 'kumuha', 'kung', 'laban', 'lahat', 'lamang', 'likod', 'lima', 'maaari', 'maaaring', 'maging', 'mahusay', 'makita', 'marami', 'marapat', 'masyado', 'may', 'mayroon', 'mga', 'minsan', 'mismo', 'mula', 'muli', 'na', 'nabanggit', 'naging', 'nagkaroon', 'nais', 'nakita', 'namin', 'napaka', 'narito', 'nasaan', 'ng', 'ngayon', 'ni', 'nila', 'nilang', 'nito', 'niya', 'niyang', 'noon', 'o', 'pa', 'paano', 'pababa', 'paggawa', 'pagitan', 'pagkakaroon', 'pagkatapos', 'palabas', 'pamamagitan', 'panahon', 'pangalawa', 'para', 'paraan', 'pareho', 'pataas', 'pero', 'pumunta', 'pumupunta', 'sa', 'saan', 'sabi', 'sabihin', 'sarili', 'sila', 'sino', 'siya', 'tatlo', 'tayo', 'tulad', 'tungkol', 'una', 'walang']\n"
     ]
    }
   ],
   "source": [
    "#remove stopwords\n",
    "import urllib.request, json \n",
    "with urllib.request.urlopen(\"https://raw.githubusercontent.com/stopwords-iso/stopwords-tl/master/stopwords-tl.json\") as url:\n",
    "    stopwords = json.loads(url.read().decode())\n",
    "    print(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data['content2'] =data['Content'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwords)]))\n",
    "train['rm_stpwrds'] = train['clean_tweet'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwords) ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [inaasahan, vice, president, jejomar, binay, t...\n",
       "1    [mar, roxas, tang, ina, tuwid, daan, daw, eh, ...\n",
       "2    [salamat, sawang, suporta, taga, makati, pagba...\n",
       "3                        [putangina, mo, binay, takbo]\n",
       "4    [binay, with, selective, amnesia, forgetting, ...\n",
       "Name: tokenize, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tokenization \n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "train['tokenize'] = train['rm_stpwrds'].apply(nltk.tokenize.WhitespaceTokenizer().tokenize) \n",
    "train['tokenize'].head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokenize</th>\n",
       "      <th>lematize_nltk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6125</th>\n",
       "      <td>[yung, nagsasabing, automatic, roxas, votes, d...</td>\n",
       "      <td>[yung, nagsasabing, automatic, roxas, vote, di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8961</th>\n",
       "      <td>[khit, wala, yang, issue, yan, e, mananalo, pr...</td>\n",
       "      <td>[khit, wala, yang, issue, yan, e, mananalo, pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>[agree, binay, bbm, prez, and, vp, nya, scary]</td>\n",
       "      <td>[agree, binay, bbm, prez, and, vp, nya, scary]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6092</th>\n",
       "      <td>[binay, nararamdaman, paghahangad, ninyo, maka...</td>\n",
       "      <td>[binay, nararamdaman, paghahangad, ninyo, maka...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4503</th>\n",
       "      <td>[don, t, cheapen, the, presidency, duterte, te...</td>\n",
       "      <td>[don, t, cheapen, the, presidency, duterte, te...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tokenize  \\\n",
       "6125  [yung, nagsasabing, automatic, roxas, votes, d...   \n",
       "8961  [khit, wala, yang, issue, yan, e, mananalo, pr...   \n",
       "347      [agree, binay, bbm, prez, and, vp, nya, scary]   \n",
       "6092  [binay, nararamdaman, paghahangad, ninyo, maka...   \n",
       "4503  [don, t, cheapen, the, presidency, duterte, te...   \n",
       "\n",
       "                                          lematize_nltk  \n",
       "6125  [yung, nagsasabing, automatic, roxas, vote, di...  \n",
       "8961  [khit, wala, yang, issue, yan, e, mananalo, pr...  \n",
       "347      [agree, binay, bbm, prez, and, vp, nya, scary]  \n",
       "6092  [binay, nararamdaman, paghahangad, ninyo, maka...  \n",
       "4503  [don, t, cheapen, the, presidency, duterte, te...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "def lema_words(text):\n",
    "  wnl=WordNetLemmatizer()\n",
    "  return[wnl.lemmatize(w) for w in text]\n",
    "\n",
    "train['lematize_nltk']=train['tokenize'].apply(lema_words)  \n",
    "train[['tokenize','lematize_nltk']].sample(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proceeding to Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOWELS = \"aeiouAEIOU\"\n",
    "CONSONANTS = \"bcdfghklmnngpqrstvwyBCDFGHKLMNNGPQRSTVWY\"\n",
    "\n",
    "\"\"\" \n",
    "\tAffixes\n",
    "\"\"\"\n",
    "PREFIX_SET = [\n",
    "\t'nakikipag', 'pakikipag',\n",
    "\t'pinakama', 'pagpapa',\n",
    "\t'pinagka', 'panganga', \n",
    "\t'makapag', 'nakapag', \n",
    "\t'tagapag', 'makipag', \n",
    "\t'nakipag', 'tigapag',\n",
    "\t'pakiki', 'magpa',\n",
    "\t'napaka', 'pinaka',\n",
    "\t'ipinag', 'pagka', \n",
    "\t'pinag', 'mapag', \n",
    "\t'mapa', 'taga', \n",
    "\t'ipag', 'tiga', \n",
    "\t'pala', 'pina', \n",
    "\t'pang', 'naka',\n",
    "\t'nang', 'mang',\n",
    "\t'sing',\n",
    "\t'ipa', 'pam',\n",
    "\t'pan', 'pag',\n",
    "\t'tag', 'mai',\n",
    "\t'mag', 'nam',\n",
    "\t'nag', 'man',\n",
    "\t'may', 'ma',\n",
    "\t'na', 'ni',\n",
    "\t'pa', 'ka',\n",
    "\t'um', 'in',\n",
    "\t'i',\n",
    "]\n",
    "\n",
    "INFIX_SET = [\n",
    "\t'um', 'in',\n",
    "]\n",
    "\n",
    "SUFFIX_SET = [\n",
    "\t'syon','dor', \n",
    "\t'ita', 'han', \n",
    "\t'hin', 'ing', \n",
    "\t'ang', 'ng', \n",
    "\t'an', 'in', \n",
    "\t'g',\n",
    "]\n",
    "\n",
    "PERIOD_FLAG = True\n",
    "PASS_FLAG = False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def check_vowel(substring):\n",
    "\t\"\"\"\n",
    "\t\tChecks if the substring is a vowel.\n",
    "\t\t\tletters: substring to be tested\n",
    "\t\treturns BOOLEAN\n",
    "\t\"\"\"\n",
    "\n",
    "\treturn all(letter in VOWELS for letter in substring)\n",
    "\n",
    "\n",
    "def check_consonant(substring):\n",
    "\t\"\"\"\n",
    "\t\tChecks if the letter is a consonant.\n",
    "\t\t\tletter: substring to be tested\n",
    "\t\treturns BOOLEAN\n",
    "\t\"\"\"\n",
    "\n",
    "\treturn all(letter in CONSONANTS for letter in substring)\n",
    "\n",
    "def change_letter(token, index, letter):\n",
    "\t\"\"\"\n",
    "\t\tReplaces a letter in a token.\n",
    "\t\t\ttoken: word to be used\n",
    "\t\t\tindex: index of the letter\n",
    "\t\t\tletter: letter used to replace\n",
    "\t\treturns STRING\n",
    "\t\"\"\"\n",
    "\t\n",
    "\t_list = list(token)\n",
    "\t_list[index] = letter\n",
    "\n",
    "\treturn ''.join(_list)\n",
    "\n",
    "def count_vowel(token):\n",
    "\t\"\"\"\n",
    "\t\tCount vowels in a given token.\n",
    "\t\t\ttoken: string to be counted for vowels\n",
    "\t\treturns INTEGER\n",
    "\t\"\"\"\n",
    "\n",
    "\tcount = 0\n",
    "\n",
    "\tfor tok in token:\n",
    "\t\tif check_vowel(tok):\n",
    "\t\t\tcount+=1\n",
    "\n",
    "\treturn count\n",
    "\n",
    "\n",
    "def count_consonant(token):\n",
    "\t\"\"\"\n",
    "\t\tCount consonants in a given token.\n",
    "\t\t\ttoken: string to be counted for consonants\n",
    "\t\treturns INTEGER\n",
    "\t\"\"\"\n",
    "\n",
    "\tcount = 0\n",
    "\n",
    "\tfor tok in token:\n",
    "\t\tif check_consonant(tok):\n",
    "\t\t\tcount+=1\n",
    "\n",
    "\treturn count\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def check_validation(token):\n",
    "    with open('stemmer/validation.txt', 'r') as valid:\n",
    "        data = valid.read().replace('\\n', ' ').split(' ')\n",
    "\n",
    "    return token in data\n",
    "\n",
    "\n",
    "\n",
    "def clean_repitition(token, REPITITION):\n",
    "\t\"\"\"\n",
    "\t\tChecks token for repitition. (ex. nakakabaliw = nabaliw)\n",
    "\t\t\ttoken: word to be stemmed repitition\n",
    "\t\treturns STRING\n",
    "\t\"\"\"\n",
    "\n",
    "\tif check_validation(token):\n",
    "\t\treturn token\n",
    "\n",
    "\tif len(token) >= 4:\n",
    "\t\tif check_vowel(token[0]):\n",
    "\t\t\tif token[0] == token[1]:\n",
    "\t\t\t\tREPITITION.append(token[0])\n",
    "\t\t\t\treturn token[1:]\n",
    "\n",
    "\t\telif check_consonant(token[0]) and count_vowel(token) >= 2:\n",
    "\t\t\tif token[0: 2] == token[2: 4] and len(token) - 2 >= 4:\n",
    "\t\t\t\tREPITITION.append(token[2:4])\n",
    "\t\t\t\treturn token[2:]\n",
    "\t\t\t\n",
    "\t\t\telif token[0: 3] == token[3: 6] and len(token) - 3 >= 4:\n",
    "\t\t\t\tREPITITION.append(token[3:6])\n",
    "\t\t\t\treturn token[3:]\n",
    "\n",
    "\treturn token\n",
    "\n",
    "def clean_suffix(token, SUFFIX):\n",
    "    \"\"\"\n",
    "    Checks token for suffixes. (ex. bigayan = bigay)\n",
    "        token: word to be stemmed for suffixes\n",
    "    returns STRING\n",
    "    \"\"\"\n",
    "\n",
    "    SUF_CANDIDATE = []\n",
    "\n",
    "    if check_validation(token):\n",
    "        return token\n",
    "\n",
    "    for suffix in SUFFIX_SET:\n",
    "        if len(token) - len(suffix) >= 3 and count_vowel(token[0:len(token) - len(suffix)]) >= 2 and count_consonant(token[0:len(token) - len(suffix)]) >= 1:\n",
    "            if token[len(token) - len(suffix): len(token)] == suffix:\n",
    "                if len(suffix) == 2 and not count_consonant(token[0:len(token) - len(suffix)]) >= 1:\n",
    "                    continue\n",
    "\n",
    "                if count_vowel(token[0: len(token) - len(suffix)]) >= 2:\n",
    "                    if suffix == 'ang' and check_consonant(token[-4]) \\\n",
    "                            and token[-4] != 'r' and token[-5] != 'u':\n",
    "                        continue\n",
    "\n",
    "                    #print(token[0: len(token) - len(suffix)] + \" : \" + suffix)\n",
    "\n",
    "                    if check_validation(token[0: len(token) - len(suffix)]):\n",
    "                        SUFFIX.append(suffix)\n",
    "                        return token[0: len(token) - len(suffix)] + 'a' if suffix == 'ita' \\\n",
    "                            else token[0: len(token) - len(suffix)]\n",
    "\n",
    "                    elif len(SUF_CANDIDATE) == 0:\n",
    "                        SUF_CANDIDATE.append(suffix)\n",
    "                        SUF_CANDIDATE.append(token[0: len(token) - len(suffix)])\n",
    "\n",
    "    if (len(SUF_CANDIDATE) == 2):\n",
    "        SUFFIX = SUF_CANDIDATE[0]\n",
    "        return SUF_CANDIDATE[1][0: len(token) - len(SUFFIX)] + 'a' if SUFFIX == 'ita' \\\n",
    "            else SUF_CANDIDATE[1][0: len(token) - len(SUFFIX)]\n",
    "\n",
    "    return token\n",
    "\n",
    "\n",
    "def clean_infix(token, INFIX):\n",
    "\t\"\"\"\n",
    "\t\tChecks token for infixes. (ex. bumalik = balik)\n",
    "\t\t\ttoken: word to be stemmed for infixes\n",
    "\t\treturns STRING\n",
    "\t\"\"\"\n",
    "\n",
    "\tif check_validation(token):\n",
    "\t\treturn token\n",
    "\n",
    "\tfor infix in INFIX_SET:\n",
    "\t\tif len(token) - len(infix) >= 3 and count_vowel(token[len(infix):]) >= 2:\n",
    "\t\t\tif token[0] == token[4] and token[1: 4] == infix:\n",
    "\t\t\t\tINFIX.append(infix)\n",
    "\t\t\t\treturn token[4:]\n",
    "\n",
    "\t\t\telif token[2] == token[4] and token[1: 3] == infix:\n",
    "\t\t\t\tINFIX.append(infix)\n",
    "\t\t\t\treturn token[0] + token[3:]\n",
    "\n",
    "\t\t\telif token[1: 3] == infix and check_vowel(token[3]):\n",
    "\t\t\t\tINFIX.append(infix)\n",
    "\t\t\t\treturn token[0] + token[3:]\n",
    "\n",
    "\treturn token\n",
    "\n",
    "\n",
    "def clean_prefix(token,\t PREFIX):\n",
    "\t\"\"\"\n",
    "\t\tChecks token for prefixes. (ex. naligo = ligo)\n",
    "\t\t\ttoken: word to be stemmed for prefixes\n",
    "\t\treturns STRING\n",
    "\t\"\"\"\n",
    "\n",
    "\tif check_validation(token):\n",
    "\t\treturn token\n",
    "\n",
    "\tfor prefix in PREFIX_SET:\n",
    "\t\tif len(token) - len(prefix) >= 3 and \\\n",
    "\t\t\tcount_vowel(token[len(prefix):]) >= 2:\n",
    "\n",
    "\t\t\tif prefix == ('i') and check_consonant(token[2]):\n",
    "\t\t\t\tcontinue\n",
    "\n",
    "\t\t\tif '-' in token:\t\n",
    "\t\t\t\ttoken = token.split('-')\n",
    "\n",
    "\t\t\t\tif token[0] == prefix and check_vowel(token[1][0]):\n",
    "\t\t\t\t\tPREFIX.append(prefix)\n",
    "\t\t\t\t\treturn token[1]\n",
    "\n",
    "\t\t\t\ttoken = '-'.join(token)\n",
    "\n",
    "\t\t\tif token[0: len(prefix)] == prefix:\n",
    "\t\t\t\tif count_vowel(token[len(prefix):]) >= 2:\n",
    "\t\t\t\t\t# if check_vowel(token[len(token) - len(prefix) - 1]):\n",
    "\t\t\t\t# \tcontinue\n",
    "\n",
    "\t\t\t\t\tif prefix == 'panganga':\n",
    "\t\t\t\t\t\tPREFIX.append(prefix)\n",
    "\t\t\t\t\t\treturn 'ka' + token[len(prefix):]\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\tPREFIX.append(prefix)\n",
    "\t\t\t\t\treturn token[len(prefix):]\n",
    "\n",
    "\treturn token\n",
    "\n",
    "\n",
    "def clean_duplication(token, DUPLICATE):\n",
    "\t\"\"\"\n",
    "\t\tChecks token for duplication. (ex. araw-araw = araw)\n",
    "\t\t\ttoken: word to be stemmed duplication\n",
    "\t\treturns STRING\n",
    "\t\"\"\"\n",
    "\n",
    "\tif check_validation(token):\n",
    "\t\treturn token\n",
    "\n",
    "\tif '-' in token and token.index('-') != 0 and \\\n",
    "\t\ttoken.index('-') != len(token) -  1:\n",
    "\n",
    "\t\tsplit = token.split('-')\n",
    "\n",
    "\t\tif all(len(tok) >= 3 for tok in split):\n",
    "\t\t\tif split[0] == token[1] or split[0][-1] == 'u' and change_letter(split[0], -1, 'o') == split[1] or \\\n",
    "\t\t\t\tsplit[0][-2] == 'u' and change_letter(split[0], -2, 'o')  == split[1]:\n",
    "\t\t\t\tDUPLICATE.append(split[0])\n",
    "\t\t\t\treturn split[0]\n",
    "\n",
    "\t\t\telif split[0] == split[1][0:len(split[0])]:\n",
    "\t\t\t\tDUPLICATE.append(split[1])\n",
    "\t\t\t\treturn split[1]\n",
    "\n",
    "\t\t\telif split[0][-2:] == 'ng':\n",
    "\t\t\t\tif split[0][-3] == 'u':\n",
    "\t\t\t\t\tif split[0][0:-3] + 'o' == split[1]:\n",
    "\t\t\t\t\t\tDUPLICATE.append(split[1])\n",
    "\t\t\t\t\t\treturn split[1]\n",
    "\n",
    "\t\t\t\tif split[0][0:-2] == split[1]:\n",
    "\t\t\t\t\tDUPLICATE.append(split[1])\n",
    "\t\t\t\t\treturn split[1]\n",
    "\n",
    "\t\telse:\n",
    "\t\t\treturn '-'.join(split)\n",
    "\t\n",
    "\treturn token\n",
    "\n",
    "\n",
    "def clean_repitition(token, REPITITION):\n",
    "\t\"\"\"\n",
    "\t\tChecks token for repitition. (ex. nakakabaliw = nabaliw)\n",
    "\t\t\ttoken: word to be stemmed repitition\n",
    "\t\treturns STRING\n",
    "\t\"\"\"\n",
    "\n",
    "\tif check_validation(token):\n",
    "\t\treturn token\n",
    "\n",
    "\tif len(token) >= 4:\n",
    "\t\tif check_vowel(token[0]):\n",
    "\t\t\tif token[0] == token[1]:\n",
    "\t\t\t\tREPITITION.append(token[0])\n",
    "\t\t\t\treturn token[1:]\n",
    "\n",
    "\t\telif check_consonant(token[0]) and count_vowel(token) >= 2:\n",
    "\t\t\tif token[0: 2] == token[2: 4] and len(token) - 2 >= 4:\n",
    "\t\t\t\tREPITITION.append(token[2:4])\n",
    "\t\t\t\treturn token[2:]\n",
    "\t\t\t\n",
    "\t\t\telif token[0: 3] == token[3: 6] and len(token) - 3 >= 4:\n",
    "\t\t\t\tREPITITION.append(token[3:6])\n",
    "\t\t\t\treturn token[3:]\n",
    "\n",
    "\treturn token\n",
    "\n",
    "\n",
    "def clean_stemmed(token, CLEANERS, REPITITION):\n",
    "\t\t\n",
    "\tif not token:\n",
    "\t\treturn \"\"\n",
    "\t\n",
    "\t\"\"\"\n",
    "\t\tChecks for left-over affixes and letters.\n",
    "\t\t\ttoken: word to be cleaned for excess affixes/letters\n",
    "\t\treturns STRING\n",
    "\t\"\"\"\n",
    "\n",
    "\tglobal PERIOD_FLAG\n",
    "\tglobal PASS_FLAG\n",
    "\n",
    "\tCC_EXP = ['dr', 'gl', 'gr', 'ng', 'kr', 'kl', 'kw', 'ts', 'tr', 'pr', 'pl', 'pw', 'sw', 'sy'] # Consonant + Consonant Exceptions\n",
    "\n",
    "\tif token[-1] == '.' and PASS_FLAG == False:\n",
    "\t\tPERIOD_FLAG = True\n",
    "\n",
    "\tif not check_vowel(token[-1]) and not check_consonant(token[-1]):\n",
    "\t\tCLEANERS.append(token[-1])\n",
    "\t\ttoken = token[0:-1]\n",
    "\n",
    "#\tif not check_vowel(token[0]) and not check_consonant(token[0]):\n",
    "#\t\tCLEANERS.append(token[0])\n",
    "#\t\ttoken = token[1:]\n",
    "\n",
    "\tif check_validation(token):\n",
    "\t\treturn token\n",
    "\n",
    "\tif len(token) >= 3 and count_vowel(token) >= 2:\n",
    "\t\ttoken = clean_repitition(token,\tREPITITION)\n",
    "\n",
    "\t\tif check_consonant(token[-1]) and token[- 2] == 'u':\n",
    "\t\t\tCLEANERS.append('u')\n",
    "\t\t\ttoken = change_letter(token, -2, 'o')\n",
    "\n",
    "\t\tif token[len(token) - 1] == 'u':\n",
    "\t\t\tCLEANERS.append('u')\n",
    "\t\t\ttoken = change_letter(token, -1, 'o')\n",
    "\n",
    "\t\tif token[-1] == 'r':\n",
    "\t\t\tCLEANERS.append('r')\n",
    "\t\t\ttoken = change_letter(token, -1, 'd')\n",
    "\n",
    "\t\tif token[-1] == 'h' and check_vowel(token[-1]):\n",
    "\t\t\tCLEANERS.append('h')\n",
    "\t\t\ttoken = token[0:-1]\n",
    "\n",
    "\t\t# if token[0] == 'i':\n",
    "\t\t# \ttoken = token[1:]\n",
    "\n",
    "\t\tif token[0] == token[1]:\n",
    "\t\t\tCLEANERS.append(token[0])\n",
    "\t\t\ttoken = token[1:]\n",
    "\n",
    "\t\tif (token[0: 2] == 'ka' or token[0: 2] == 'pa') and check_consonant(token[2]) \\\n",
    "\t\t\tand count_vowel(token) >= 3:\n",
    "\t\t\t\n",
    "\t\t\tCLEANERS.append(token[0: 2])\n",
    "\t\t\ttoken = token[2:]\n",
    "\n",
    "\t\tif(token[-3:]) == 'han' and count_vowel(token[0:-3]) == 1:\n",
    "\t\t\tCLEANERS.append('han')\n",
    "\t\t\ttoken = token[0:-3] + 'i'\n",
    "\n",
    "\t\tif(token[-3:]) == 'han' and count_vowel(token[0:-3]) > 1:\n",
    "\t\t\tCLEANERS.append('han')\n",
    "\t\t\ttoken = token[0:-3]\n",
    "\n",
    "\t\tif len(token) >= 2 and count_vowel(token) >= 3:\n",
    "\t\t\tif token[-1] == 'h' and check_vowel(token[-2]):\n",
    "\t\t\t\tCLEANERS.append('h')\n",
    "\t\t\t\ttoken = token[0:-1]\n",
    "\n",
    "\t\tif len(token) >= 6 and token[0:2] == token[2:4]:\n",
    "\t\t\tCLEANERS.append('0:2')\n",
    "\t\t\ttoken = token[2:]\n",
    "\n",
    "\t\tif any(REP[0] == 'r' for REP in REPITITION):\n",
    "\t\t\tCLEANERS.append('r')\n",
    "\t\t\ttoken = change_letter(token, 0, 'd')\n",
    "\n",
    "\t\tif token[-2:] == 'ng' and token[-3] == 'u':\n",
    "\t\t\tCLEANERS.append('u')\n",
    "\t\t\ttoken = change_letter(token, -3, 'o')\n",
    "\n",
    "\t\tif token[-1] == 'h':\n",
    "\t\t\tCLEANERS.append('h')\n",
    "\t\t\ttoken = token[0:-1]\n",
    "\n",
    "\t\tif any(token[0:2] != CC for CC in CC_EXP) and check_consonant(token[0:2]):\n",
    "\t\t\tCLEANERS.append(token[0:2])\n",
    "\t\t\ttoken = token[1:]\n",
    "\n",
    "\treturn token\n",
    "\n",
    "\n",
    "def tg_stemmer(tokens):\n",
    "\n",
    "    global PERIOD_FLAG\n",
    "    global PASS_FLAG\n",
    "\n",
    "    pre_stem     = inf_stem = suf_stem = rep_stem = \\\n",
    "        du1_stem = du2_stem = cle_stem = '-'\n",
    "    word_info    = {}\n",
    "    PREFIX     = []\n",
    "    INFIX      = []\n",
    "    SUFFIX     = []\n",
    "    DUPLICATE  = []\n",
    "    REPITITION = []\n",
    "    CLEANERS   = []\n",
    "\n",
    "    word_info['clean'] = '-'\n",
    "    stemmed_tokens = []\n",
    "\n",
    "    for token in tokens:\n",
    "        word_info = {}\n",
    "        word_info[\"word\"] = token\n",
    "\n",
    "        if (PERIOD_FLAG == True and token[0].isupper()) or \\\n",
    "                (PERIOD_FLAG == False and token[0].islower()):\n",
    "            token = token.lower()\n",
    "            du1_stem = clean_duplication(token, DUPLICATE)\n",
    "            pre_stem = clean_prefix(du1_stem, PREFIX)\n",
    "            rep_stem = clean_repitition(pre_stem, REPITITION)\n",
    "            inf_stem = clean_infix(rep_stem, INFIX)\n",
    "            rep_stem = clean_repitition(inf_stem, REPITITION)\n",
    "            suf_stem = clean_suffix(rep_stem, SUFFIX)\n",
    "            du2_stem = clean_duplication(suf_stem, DUPLICATE)\n",
    "            cle_stem = clean_stemmed(du2_stem, CLEANERS, REPITITION)\n",
    "            cle_stem = clean_duplication(cle_stem, DUPLICATE)\n",
    "\n",
    "            if '-' in cle_stem:\n",
    "                cle_stem.replace('-', '')\n",
    "\n",
    "        else:\n",
    "            PERIOD_FLAG = False\n",
    "            cle_stem = clean_stemmed(token, CLEANERS, REPITITION)\n",
    "            word_info[\"root\"]   = token\n",
    "            word_info[\"prefix\"] = '[]'\n",
    "            word_info[\"infix\"]  = '[]'\n",
    "            word_info[\"suffix\"] = '[]'\n",
    "            word_info[\"repeat\"] = '[]'\n",
    "            word_info[\"dupli\"]  = '[]'\n",
    "            word_info[\"clean\"]  = cle_stem\n",
    "\n",
    "        stemmed_tokens.append(cle_stem)\n",
    "\n",
    "    return stemmed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ENGLISH STEMMER\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def english_lemmatizer(token):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return lemmatizer.lemmatize(token)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\jmest\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_tagalog_english(tokens):\n",
    "    lemmatized_tokens = []\n",
    "\n",
    "    for token in tokens:\n",
    "        if wordnet.synsets(token):\n",
    "            pos = nltk.pos_tag([token])[0][1][0].lower()\n",
    "            pos = {'a': wordnet.ADJ,\n",
    "                   'n': wordnet.NOUN,\n",
    "                   'v': wordnet.VERB,\n",
    "                   'r': wordnet.ADV}.get(pos, wordnet.NOUN)\n",
    "            lemmatized_token = lemmatizer.lemmatize(token, pos)\n",
    "        else:\n",
    "            lemmatized_token = tg_stemmer([token])[0]\n",
    "        \n",
    "        lemmatized_tokens.append(lemmatized_token)\n",
    "\n",
    "    return lemmatized_tokens\n",
    "\n",
    "train['lemmatize'] = train['tokenize'].apply(lemmatize_tagalog_english)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>rm_stpwrds</th>\n",
       "      <th>tokenize</th>\n",
       "      <th>lemmatize</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9496</th>\n",
       "      <td>1</td>\n",
       "      <td>The desperate move of Roxas to ask Poe to \"uni...</td>\n",
       "      <td>the desperate move of roxas to ask poe to unit...</td>\n",
       "      <td>[the, desperate, move, of, roxas, to, ask, poe...</td>\n",
       "      <td>[the, desperate, move, of, roxas, to, ask, poe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9497</th>\n",
       "      <td>1</td>\n",
       "      <td>After Sinulog, kani na sad.. binay ikaw na.. a...</td>\n",
       "      <td>after sinulog kani sad binay pinakamakapal</td>\n",
       "      <td>[after, sinulog, kani, sad, binay, pinakamakapal]</td>\n",
       "      <td>[after, sulo, kani, sad, binay, kapal]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9498</th>\n",
       "      <td>1</td>\n",
       "      <td>Wag na wag ko lang malalaman na buhay ni Mar R...</td>\n",
       "      <td>wag wag lang malalaman buhay mar roxas susunod...</td>\n",
       "      <td>[wag, wag, lang, malalaman, buhay, mar, roxas,...</td>\n",
       "      <td>[wag, wag, lang, laman, buhay, mar, roxas, sun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9499</th>\n",
       "      <td>0</td>\n",
       "      <td>Kung di man si duterte, Miriam nalang presiden...</td>\n",
       "      <td>di man si duterte miriam nalang president mar ...</td>\n",
       "      <td>[di, man, si, duterte, miriam, nalang, preside...</td>\n",
       "      <td>[di, man, si, duterte, miriam, nala, president...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9500</th>\n",
       "      <td>0</td>\n",
       "      <td>You can't make me watch Delta aaaaaaaaaaaaaaaaa</td>\n",
       "      <td>you can t make me watch delta aaaaaaaaaaaaaaaaa</td>\n",
       "      <td>[you, can, t, make, me, watch, delta, aaaaaaaa...</td>\n",
       "      <td>[yoo, can, t, make, me, watch, delta, aaaaaaaa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9501</th>\n",
       "      <td>1</td>\n",
       "      <td>kaninang pa itong Binay binay binay.....tch</td>\n",
       "      <td>kaninang itong binay binay binay tch</td>\n",
       "      <td>[kaninang, itong, binay, binay, binay, tch]</td>\n",
       "      <td>[nina, ito, binay, binay, binay, tch]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9502</th>\n",
       "      <td>0</td>\n",
       "      <td>Mar Roxas on the rise, w/ momentum, machinery,...</td>\n",
       "      <td>mar roxas on the rise w momentum machinery gra...</td>\n",
       "      <td>[mar, roxas, on, the, rise, w, momentum, machi...</td>\n",
       "      <td>[mar, roxas, on, the, rise, w, momentum, machi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9503</th>\n",
       "      <td>0</td>\n",
       "      <td>Alan Cayetano 'confirms' Palace, Roxas, Poe be...</td>\n",
       "      <td>alan cayetano confirms palace roxas poe behind...</td>\n",
       "      <td>[alan, cayetano, confirms, palace, roxas, poe,...</td>\n",
       "      <td>[alan, cayetano, confirms, palace, roxas, poe,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9504</th>\n",
       "      <td>1</td>\n",
       "      <td>Mas nakakainis ad ni Mar kaysa kay Binay.</td>\n",
       "      <td>mas nakakainis ad mar kay binay</td>\n",
       "      <td>[mas, nakakainis, ad, mar, kay, binay]</td>\n",
       "      <td>[ma, kainis, ad, mar, kay, binay]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9505</th>\n",
       "      <td>0</td>\n",
       "      <td>Preliminary and partial results coming in sugg...</td>\n",
       "      <td>preliminary and partial results coming in sugg...</td>\n",
       "      <td>[preliminary, and, partial, results, coming, i...</td>\n",
       "      <td>[preliminary, and, partial, result, come, in, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                               text  \\\n",
       "9496      1  The desperate move of Roxas to ask Poe to \"uni...   \n",
       "9497      1  After Sinulog, kani na sad.. binay ikaw na.. a...   \n",
       "9498      1  Wag na wag ko lang malalaman na buhay ni Mar R...   \n",
       "9499      0  Kung di man si duterte, Miriam nalang presiden...   \n",
       "9500      0    You can't make me watch Delta aaaaaaaaaaaaaaaaa   \n",
       "9501      1        kaninang pa itong Binay binay binay.....tch   \n",
       "9502      0  Mar Roxas on the rise, w/ momentum, machinery,...   \n",
       "9503      0  Alan Cayetano 'confirms' Palace, Roxas, Poe be...   \n",
       "9504      1          Mas nakakainis ad ni Mar kaysa kay Binay.   \n",
       "9505      0  Preliminary and partial results coming in sugg...   \n",
       "\n",
       "                                             rm_stpwrds  \\\n",
       "9496  the desperate move of roxas to ask poe to unit...   \n",
       "9497         after sinulog kani sad binay pinakamakapal   \n",
       "9498  wag wag lang malalaman buhay mar roxas susunod...   \n",
       "9499  di man si duterte miriam nalang president mar ...   \n",
       "9500    you can t make me watch delta aaaaaaaaaaaaaaaaa   \n",
       "9501               kaninang itong binay binay binay tch   \n",
       "9502  mar roxas on the rise w momentum machinery gra...   \n",
       "9503  alan cayetano confirms palace roxas poe behind...   \n",
       "9504                    mas nakakainis ad mar kay binay   \n",
       "9505  preliminary and partial results coming in sugg...   \n",
       "\n",
       "                                               tokenize  \\\n",
       "9496  [the, desperate, move, of, roxas, to, ask, poe...   \n",
       "9497  [after, sinulog, kani, sad, binay, pinakamakapal]   \n",
       "9498  [wag, wag, lang, malalaman, buhay, mar, roxas,...   \n",
       "9499  [di, man, si, duterte, miriam, nalang, preside...   \n",
       "9500  [you, can, t, make, me, watch, delta, aaaaaaaa...   \n",
       "9501        [kaninang, itong, binay, binay, binay, tch]   \n",
       "9502  [mar, roxas, on, the, rise, w, momentum, machi...   \n",
       "9503  [alan, cayetano, confirms, palace, roxas, poe,...   \n",
       "9504             [mas, nakakainis, ad, mar, kay, binay]   \n",
       "9505  [preliminary, and, partial, results, coming, i...   \n",
       "\n",
       "                                              lemmatize  \n",
       "9496  [the, desperate, move, of, roxas, to, ask, poe...  \n",
       "9497             [after, sulo, kani, sad, binay, kapal]  \n",
       "9498  [wag, wag, lang, laman, buhay, mar, roxas, sun...  \n",
       "9499  [di, man, si, duterte, miriam, nala, president...  \n",
       "9500  [yoo, can, t, make, me, watch, delta, aaaaaaaa...  \n",
       "9501              [nina, ito, binay, binay, binay, tch]  \n",
       "9502  [mar, roxas, on, the, rise, w, momentum, machi...  \n",
       "9503  [alan, cayetano, confirms, palace, roxas, poe,...  \n",
       "9504                  [ma, kainis, ad, mar, kay, binay]  \n",
       "9505  [preliminary, and, partial, result, come, in, ...  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[['label','text','rm_stpwrds','tokenize','lemmatize']].tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(train['lemmatize'], train['label'], test_size=0.3, random_state=1)\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(map(' '.join, X_train))\n",
    "X_test = vectorizer.transform(map(' '.join, X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7433380084151473\n"
     ]
    }
   ],
   "source": [
    "# Train the SVM model\n",
    "clf = svm.SVC()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Dump the trained SVM model to a file using joblib\n",
    "joblib.dump(clf, 'svm_model.joblib')\n",
    "joblib.dump(vectorizer, 'tfidf_vectorizer.joblib')\n",
    "\n",
    "# Test the model on the testing set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "accuracy = clf.score(X_test, y_test)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9667868951006913\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model's performance\n",
    "accuracy = clf.score(X_train, y_train)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tfidf_vectorizer.joblib']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(clf, 'svm_model.joblib')\n",
    "\n",
    "joblib.dump(vectorizer, 'tfidf_vectorizer.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tangina mo jepoy dizon ang baho baho mo\n",
      "Negative Statement\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "import urllib.request, json\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "# Download Filipino stopwords\n",
    "with urllib.request.urlopen(\"https://raw.githubusercontent.com/stopwords-iso/stopwords-tl/master/stopwords-tl.json\") as url:\n",
    "    stopwords_tl = json.loads(url.read().decode())\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Remove unwanted characters and digits\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    #Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Convert tokens to lowercase\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    # # Remove stopwords\n",
    "    tokens = [word for word in tokens if word.lower() not in stopwords_tl]\n",
    "    # Lemmatize the tokens\n",
    "    lemmatized_tokens = []\n",
    "\n",
    "    for token in tokens:\n",
    "        if wordnet.synsets(token):\n",
    "            pos = nltk.pos_tag([token])[0][1][0].lower()\n",
    "            pos = {'a': wordnet.ADJ,\n",
    "                   'n': wordnet.NOUN,\n",
    "                   'v': wordnet.VERB,\n",
    "                   'r': wordnet.ADV}.get(pos, wordnet.NOUN)\n",
    "            lemmatized_token = lemmatizer.lemmatize(token, pos)\n",
    "        else:\n",
    "            lemmatized_token = tg_stemmer([token])[0]\n",
    "        \n",
    "        lemmatized_tokens.append(lemmatized_token)\n",
    "        \n",
    "    # Join the tokens back into a string\n",
    "    text = ' '.join(lemmatized_tokens)\n",
    "    return text\n",
    "\n",
    "\n",
    "# Load the trained SVM model\n",
    "clf = joblib.load('svm_model.joblib')\n",
    "\n",
    "# Load the vectorizer fitted on the training data\n",
    "vectorizer = joblib.load('tfidf_vectorizer.joblib')\n",
    "\n",
    "# Get input from user\n",
    "sentiment = input(\"Enter a sentence to analyze: \")\n",
    "\n",
    "# Preprocess the input text\n",
    "sentiment_processed = preprocess_text(sentiment)\n",
    "\n",
    "# Vectorize the input text\n",
    "sentiment_vectorized = vectorizer.transform([sentiment_processed])\n",
    "\n",
    "# Predict the sentiment using the trained SVM model\n",
    "prediction = clf.predict(sentiment_vectorized)\n",
    "\n",
    "print(sentiment)\n",
    "# Print the prediction\n",
    "if prediction == 1:\n",
    "    print(\"Negative Statement\")\n",
    "else:\n",
    "    print(\"Positive sentiment\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
