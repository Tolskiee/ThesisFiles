{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import neccessary dependencies\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data\n",
    "#train = pd.read_csv(r\"C:\\Users\\Admin\\OneDrive\\Documents\\Webdev\\hatespeech\\train.csv\", nrows=5000)\n",
    "\n",
    "# testing data\n",
    "#test = pd.read_csv(r\"C:\\Users\\Admin\\OneDrive\\Documents\\Webdev\\hatespeech\\test.csv\", nrows=5000)\n",
    "\n",
    "\n",
    "# #training data\n",
    "train = pd.read_csv(r\"C:\\Users\\jmest\\Documents\\Files\\C4S2\\Thesis\\ThesisFiles-main\\ThesisFiles-main\\hatespeech\\train.csv\", nrows=100)\n",
    "\n",
    "# #testing data\n",
    "test = pd.read_csv(r\"C:\\Users\\jmest\\Documents\\Files\\C4S2\\Thesis\\ThesisFiles-main\\ThesisFiles-main\\hatespeech\\test.csv\", nrows=5000)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean the data of null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Inaasahan na ni Vice President Jejomar Binay n...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mar Roxas TANG INA TUWID NA DAAN DAW .. EH SYA...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Salamat sa walang sawang suporta ng mga taga m...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@rapplerdotcom putangina mo binay TAKBO PA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Binay with selective amnesia, forgetting about...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Tangina ka Enrile. Di naman masyadong halatang...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>@suigeneris_05 Errata: Binay pa rin ang may pi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Introduction mo Mar Roxas panay paninira lang ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>I VOTE ROXAS #Halalan2016 Mar Daang Matuwid RO...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Nakakairita commercial ni Binay!!!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  label\n",
       "0   Inaasahan na ni Vice President Jejomar Binay n...      0\n",
       "1   Mar Roxas TANG INA TUWID NA DAAN DAW .. EH SYA...      1\n",
       "2   Salamat sa walang sawang suporta ng mga taga m...      0\n",
       "3          @rapplerdotcom putangina mo binay TAKBO PA      1\n",
       "4   Binay with selective amnesia, forgetting about...      0\n",
       "..                                                ...    ...\n",
       "95  Tangina ka Enrile. Di naman masyadong halatang...      1\n",
       "96  @suigeneris_05 Errata: Binay pa rin ang may pi...      1\n",
       "97  Introduction mo Mar Roxas panay paninira lang ...      1\n",
       "98  I VOTE ROXAS #Halalan2016 Mar Daang Matuwid RO...      0\n",
       "99                 Nakakairita commercial ni Binay!!!      1\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop rows with null values\n",
    "train.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text     0\n",
       "label    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check for null values in train\n",
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for 0 values in train\n",
    "sum(train[\"label\"] == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for 1 values in train\n",
    "sum(train[\"label\"] == 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean the data of unwated Text and Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove special characters using the regular expression library\n",
    "\n",
    "import re\n",
    "\n",
    "#set up punctuations we want to be replaced\n",
    "\n",
    "REPLACE_NO_SPACE = re.compile(\"(\\.)|(\\;)|(\\:)|(\\!)|(\\')|(\\?)|(\\,)|(\\\")|(\\|)|(\\()|(\\))|(\\[)|(\\])|(\\%)|(\\$)|(\\>)|(\\<)|(\\{)|(\\})\")\n",
    "REPLACE_WITH_SPACE = re.compile(\"(<br\\s/><br\\s/?)|(-)|(/)|(:).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import preprocessor as p\n",
    "\n",
    "# custum function to clean the dataset (combining tweet_preprocessor and reguar expression)\n",
    "def clean_tweets(df):\n",
    "    tempArr = []\n",
    "    for line in df:\n",
    "        # send to tweet_processor\n",
    "        tmpL = p.clean(line)\n",
    "        # remove everything except letters and digits\n",
    "        tmpL = re.sub(r'[^a-zA-Z0-9\\s]', ' ', tmpL)\n",
    "        # convert to lowercase\n",
    "        tmpL = tmpL.lower()\n",
    "        tempArr.append(tmpL)\n",
    "    return tempArr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean training data\n",
    "train_tweet = clean_tweets(train[\"text\"])\n",
    "train_tweet = pd.DataFrame(train_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>clean_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Inaasahan na ni Vice President Jejomar Binay n...</td>\n",
       "      <td>0</td>\n",
       "      <td>inaasahan na ni vice president jejomar binay n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mar Roxas TANG INA TUWID NA DAAN DAW .. EH SYA...</td>\n",
       "      <td>1</td>\n",
       "      <td>mar roxas tang ina tuwid na daan daw    eh sya...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Salamat sa walang sawang suporta ng mga taga m...</td>\n",
       "      <td>0</td>\n",
       "      <td>salamat sa walang sawang suporta ng mga taga m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@rapplerdotcom putangina mo binay TAKBO PA</td>\n",
       "      <td>1</td>\n",
       "      <td>putangina mo binay takbo pa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Binay with selective amnesia, forgetting about...</td>\n",
       "      <td>0</td>\n",
       "      <td>binay with selective amnesia  forgetting about...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>It doesn't matter whoever won between Duterte ...</td>\n",
       "      <td>0</td>\n",
       "      <td>it doesn t matter whoever won between duterte ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Nognog? Pero nognog din ang nag malasakit? Wtf...</td>\n",
       "      <td>1</td>\n",
       "      <td>nognog  pero nognog din ang nag malasakit  wtf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>#OnlyB1nay ?? #FB https://t.co/QEQnsK67Gm</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>What Abi Binay said on running for Makati mayo...</td>\n",
       "      <td>0</td>\n",
       "      <td>what abi binay said on running for makati mayor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Srsly. How can Binay do away with no tax for t...</td>\n",
       "      <td>1</td>\n",
       "      <td>srsly  how can binay do away with no tax for t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label  \\\n",
       "0  Inaasahan na ni Vice President Jejomar Binay n...      0   \n",
       "1  Mar Roxas TANG INA TUWID NA DAAN DAW .. EH SYA...      1   \n",
       "2  Salamat sa walang sawang suporta ng mga taga m...      0   \n",
       "3         @rapplerdotcom putangina mo binay TAKBO PA      1   \n",
       "4  Binay with selective amnesia, forgetting about...      0   \n",
       "5  It doesn't matter whoever won between Duterte ...      0   \n",
       "6  Nognog? Pero nognog din ang nag malasakit? Wtf...      1   \n",
       "7          #OnlyB1nay ?? #FB https://t.co/QEQnsK67Gm      1   \n",
       "8  What Abi Binay said on running for Makati mayo...      0   \n",
       "9  Srsly. How can Binay do away with no tax for t...      1   \n",
       "\n",
       "                                         clean_tweet  \n",
       "0  inaasahan na ni vice president jejomar binay n...  \n",
       "1  mar roxas tang ina tuwid na daan daw    eh sya...  \n",
       "2  salamat sa walang sawang suporta ng mga taga m...  \n",
       "3                        putangina mo binay takbo pa  \n",
       "4  binay with selective amnesia  forgetting about...  \n",
       "5  it doesn t matter whoever won between duterte ...  \n",
       "6  nognog  pero nognog din ang nag malasakit  wtf...  \n",
       "7                                                     \n",
       "8    what abi binay said on running for makati mayor  \n",
       "9  srsly  how can binay do away with no tax for t...  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# append cleaned tweets to the training data\n",
    "train[\"clean_tweet\"] = train_tweet\n",
    "\n",
    "# compare the cleaned and uncleaned tweets\n",
    "train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(train['clean_tweet'] == '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['Another'] = df['Another'].replace('', np.nan)\n",
    "#replace all empty spaces with NaN to drop using dropna\n",
    "train['clean_tweet'] = train['clean_tweet'].replace('', np.NaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>clean_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Inaasahan na ni Vice President Jejomar Binay n...</td>\n",
       "      <td>0</td>\n",
       "      <td>inaasahan na ni vice president jejomar binay n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mar Roxas TANG INA TUWID NA DAAN DAW .. EH SYA...</td>\n",
       "      <td>1</td>\n",
       "      <td>mar roxas tang ina tuwid na daan daw    eh sya...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Salamat sa walang sawang suporta ng mga taga m...</td>\n",
       "      <td>0</td>\n",
       "      <td>salamat sa walang sawang suporta ng mga taga m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@rapplerdotcom putangina mo binay TAKBO PA</td>\n",
       "      <td>1</td>\n",
       "      <td>putangina mo binay takbo pa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Binay with selective amnesia, forgetting about...</td>\n",
       "      <td>0</td>\n",
       "      <td>binay with selective amnesia  forgetting about...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Tangina ka Enrile. Di naman masyadong halatang...</td>\n",
       "      <td>1</td>\n",
       "      <td>tangina ka enrile  di naman masyadong halatang...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>@suigeneris_05 Errata: Binay pa rin ang may pi...</td>\n",
       "      <td>1</td>\n",
       "      <td>errata  binay pa rin ang may pinakamataas na d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Introduction mo Mar Roxas panay paninira lang ...</td>\n",
       "      <td>1</td>\n",
       "      <td>introduction mo mar roxas panay paninira lang ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>I VOTE ROXAS #Halalan2016 Mar Daang Matuwid RO...</td>\n",
       "      <td>0</td>\n",
       "      <td>i vote roxas mar daang matuwid roro rising all...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Nakakairita commercial ni Binay!!!</td>\n",
       "      <td>1</td>\n",
       "      <td>nakakairita commercial ni binay</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>98 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  label  \\\n",
       "0   Inaasahan na ni Vice President Jejomar Binay n...      0   \n",
       "1   Mar Roxas TANG INA TUWID NA DAAN DAW .. EH SYA...      1   \n",
       "2   Salamat sa walang sawang suporta ng mga taga m...      0   \n",
       "3          @rapplerdotcom putangina mo binay TAKBO PA      1   \n",
       "4   Binay with selective amnesia, forgetting about...      0   \n",
       "..                                                ...    ...   \n",
       "95  Tangina ka Enrile. Di naman masyadong halatang...      1   \n",
       "96  @suigeneris_05 Errata: Binay pa rin ang may pi...      1   \n",
       "97  Introduction mo Mar Roxas panay paninira lang ...      1   \n",
       "98  I VOTE ROXAS #Halalan2016 Mar Daang Matuwid RO...      0   \n",
       "99                 Nakakairita commercial ni Binay!!!      1   \n",
       "\n",
       "                                          clean_tweet  \n",
       "0   inaasahan na ni vice president jejomar binay n...  \n",
       "1   mar roxas tang ina tuwid na daan daw    eh sya...  \n",
       "2   salamat sa walang sawang suporta ng mga taga m...  \n",
       "3                         putangina mo binay takbo pa  \n",
       "4   binay with selective amnesia  forgetting about...  \n",
       "..                                                ...  \n",
       "95  tangina ka enrile  di naman masyadong halatang...  \n",
       "96  errata  binay pa rin ang may pinakamataas na d...  \n",
       "97  introduction mo mar roxas panay paninira lang ...  \n",
       "98  i vote roxas mar daang matuwid roro rising all...  \n",
       "99                 nakakairita commercial ni binay     \n",
       "\n",
       "[98 rows x 3 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.dropna(axis='rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://statisticsglobe.com/drop-rows-blank-values-from-pandas-dataframe-python\n",
    "train['clean_tweet'] = train['clean_tweet'].replace('', float('NaN'), regex = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.dropna(inplace= True)\n",
    "train = train.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_column = train.pop('label')\n",
    "train.insert(0,'label',first_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>clean_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Inaasahan na ni Vice President Jejomar Binay n...</td>\n",
       "      <td>inaasahan na ni vice president jejomar binay n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Mar Roxas TANG INA TUWID NA DAAN DAW .. EH SYA...</td>\n",
       "      <td>mar roxas tang ina tuwid na daan daw    eh sya...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Salamat sa walang sawang suporta ng mga taga m...</td>\n",
       "      <td>salamat sa walang sawang suporta ng mga taga m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>@rapplerdotcom putangina mo binay TAKBO PA</td>\n",
       "      <td>putangina mo binay takbo pa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Binay with selective amnesia, forgetting about...</td>\n",
       "      <td>binay with selective amnesia  forgetting about...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>It doesn't matter whoever won between Duterte ...</td>\n",
       "      <td>it doesn t matter whoever won between duterte ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>Nognog? Pero nognog din ang nag malasakit? Wtf...</td>\n",
       "      <td>nognog  pero nognog din ang nag malasakit  wtf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>#OnlyB1nay ?? #FB https://t.co/QEQnsK67Gm</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>What Abi Binay said on running for Makati mayo...</td>\n",
       "      <td>what abi binay said on running for makati mayor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>Srsly. How can Binay do away with no tax for t...</td>\n",
       "      <td>srsly  how can binay do away with no tax for t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text  \\\n",
       "0      0  Inaasahan na ni Vice President Jejomar Binay n...   \n",
       "1      1  Mar Roxas TANG INA TUWID NA DAAN DAW .. EH SYA...   \n",
       "2      0  Salamat sa walang sawang suporta ng mga taga m...   \n",
       "3      1         @rapplerdotcom putangina mo binay TAKBO PA   \n",
       "4      0  Binay with selective amnesia, forgetting about...   \n",
       "5      0  It doesn't matter whoever won between Duterte ...   \n",
       "6      1  Nognog? Pero nognog din ang nag malasakit? Wtf...   \n",
       "7      1          #OnlyB1nay ?? #FB https://t.co/QEQnsK67Gm   \n",
       "8      0  What Abi Binay said on running for Makati mayo...   \n",
       "9      1  Srsly. How can Binay do away with no tax for t...   \n",
       "\n",
       "                                         clean_tweet  \n",
       "0  inaasahan na ni vice president jejomar binay n...  \n",
       "1  mar roxas tang ina tuwid na daan daw    eh sya...  \n",
       "2  salamat sa walang sawang suporta ng mga taga m...  \n",
       "3                        putangina mo binay takbo pa  \n",
       "4  binay with selective amnesia  forgetting about...  \n",
       "5  it doesn t matter whoever won between duterte ...  \n",
       "6  nognog  pero nognog din ang nag malasakit  wtf...  \n",
       "7                                                     \n",
       "8    what abi binay said on running for makati mayor  \n",
       "9  srsly  how can binay do away with no tax for t...  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(98, 3)\n"
     ]
    }
   ],
   "source": [
    "#total data entries for training\n",
    "\n",
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['akin', 'aking', 'ako', 'alin', 'am', 'amin', 'aming', 'ang', 'ano', 'anumang', 'apat', 'at', 'atin', 'ating', 'ay', 'bababa', 'bago', 'bakit', 'bawat', 'bilang', 'dahil', 'dalawa', 'dapat', 'din', 'dito', 'doon', 'gagawin', 'gayunman', 'ginagawa', 'ginawa', 'ginawang', 'gumawa', 'gusto', 'habang', 'hanggang', 'hindi', 'huwag', 'iba', 'ibaba', 'ibabaw', 'ibig', 'ikaw', 'ilagay', 'ilalim', 'ilan', 'inyong', 'isa', 'isang', 'itaas', 'ito', 'iyo', 'iyon', 'iyong', 'ka', 'kahit', 'kailangan', 'kailanman', 'kami', 'kanila', 'kanilang', 'kanino', 'kanya', 'kanyang', 'kapag', 'kapwa', 'karamihan', 'katiyakan', 'katulad', 'kaya', 'kaysa', 'ko', 'kong', 'kulang', 'kumuha', 'kung', 'laban', 'lahat', 'lamang', 'likod', 'lima', 'maaari', 'maaaring', 'maging', 'mahusay', 'makita', 'marami', 'marapat', 'masyado', 'may', 'mayroon', 'mga', 'minsan', 'mismo', 'mula', 'muli', 'na', 'nabanggit', 'naging', 'nagkaroon', 'nais', 'nakita', 'namin', 'napaka', 'narito', 'nasaan', 'ng', 'ngayon', 'ni', 'nila', 'nilang', 'nito', 'niya', 'niyang', 'noon', 'o', 'pa', 'paano', 'pababa', 'paggawa', 'pagitan', 'pagkakaroon', 'pagkatapos', 'palabas', 'pamamagitan', 'panahon', 'pangalawa', 'para', 'paraan', 'pareho', 'pataas', 'pero', 'pumunta', 'pumupunta', 'sa', 'saan', 'sabi', 'sabihin', 'sarili', 'sila', 'sino', 'siya', 'tatlo', 'tayo', 'tulad', 'tungkol', 'una', 'walang']\n"
     ]
    }
   ],
   "source": [
    "#remove stopwords\n",
    "import urllib.request, json \n",
    "with urllib.request.urlopen(\"https://raw.githubusercontent.com/stopwords-iso/stopwords-tl/master/stopwords-tl.json\") as url:\n",
    "    stopwords = json.loads(url.read().decode())\n",
    "    print(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data['content2'] =data['Content'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwords)]))\n",
    "train['rm_stpwrds'] = train['clean_tweet'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwords) ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_tweet</th>\n",
       "      <th>rm_stpwrds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>inaasahan na ni vice president jejomar binay n...</td>\n",
       "      <td>inaasahan vice president jejomar binay taong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mar roxas tang ina tuwid na daan daw    eh sya...</td>\n",
       "      <td>mar roxas tang ina tuwid daan daw eh sya nga d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>salamat sa walang sawang suporta ng mga taga m...</td>\n",
       "      <td>salamat sawang suporta taga makati pagbabalik ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>putangina mo binay takbo pa</td>\n",
       "      <td>putangina mo binay takbo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>binay with selective amnesia  forgetting about...</td>\n",
       "      <td>binay with selective amnesia forgetting about ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>it doesn t matter whoever won between duterte ...</td>\n",
       "      <td>it doesn t matter whoever won between duterte ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>nognog  pero nognog din ang nag malasakit  wtf...</td>\n",
       "      <td>nognog nognog nag malasakit wtf tangina mo bin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>what abi binay said on running for makati mayor</td>\n",
       "      <td>what abi binay said on running for makati mayor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>srsly  how can binay do away with no tax for t...</td>\n",
       "      <td>srsly how can binay do away with no tax for th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         clean_tweet  \\\n",
       "0  inaasahan na ni vice president jejomar binay n...   \n",
       "1  mar roxas tang ina tuwid na daan daw    eh sya...   \n",
       "2  salamat sa walang sawang suporta ng mga taga m...   \n",
       "3                        putangina mo binay takbo pa   \n",
       "4  binay with selective amnesia  forgetting about...   \n",
       "5  it doesn t matter whoever won between duterte ...   \n",
       "6  nognog  pero nognog din ang nag malasakit  wtf...   \n",
       "7                                                      \n",
       "8    what abi binay said on running for makati mayor   \n",
       "9  srsly  how can binay do away with no tax for t...   \n",
       "\n",
       "                                          rm_stpwrds  \n",
       "0       inaasahan vice president jejomar binay taong  \n",
       "1  mar roxas tang ina tuwid daan daw eh sya nga d...  \n",
       "2  salamat sawang suporta taga makati pagbabalik ...  \n",
       "3                           putangina mo binay takbo  \n",
       "4  binay with selective amnesia forgetting about ...  \n",
       "5  it doesn t matter whoever won between duterte ...  \n",
       "6  nognog nognog nag malasakit wtf tangina mo bin...  \n",
       "7                                                     \n",
       "8    what abi binay said on running for makati mayor  \n",
       "9  srsly how can binay do away with no tax for th...  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[['clean_tweet', 'rm_stpwrds']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [inaasahan, vice, president, jejomar, binay, t...\n",
       "1    [mar, roxas, tang, ina, tuwid, daan, daw, eh, ...\n",
       "2    [salamat, sawang, suporta, taga, makati, pagba...\n",
       "3                        [putangina, mo, binay, takbo]\n",
       "4    [binay, with, selective, amnesia, forgetting, ...\n",
       "Name: tokenize, dtype: object"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tokenization \n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "train['tokenize'] = train['rm_stpwrds'].apply(nltk.tokenize.WhitespaceTokenizer().tokenize) \n",
    "train['tokenize'].head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rm_stpwrds</th>\n",
       "      <th>tokenize</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>inaasahan vice president jejomar binay taong</td>\n",
       "      <td>[inaasahan, vice, president, jejomar, binay, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mar roxas tang ina tuwid daan daw eh sya nga d...</td>\n",
       "      <td>[mar, roxas, tang, ina, tuwid, daan, daw, eh, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>salamat sawang suporta taga makati pagbabalik ...</td>\n",
       "      <td>[salamat, sawang, suporta, taga, makati, pagba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>putangina mo binay takbo</td>\n",
       "      <td>[putangina, mo, binay, takbo]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>binay with selective amnesia forgetting about ...</td>\n",
       "      <td>[binay, with, selective, amnesia, forgetting, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>it doesn t matter whoever won between duterte ...</td>\n",
       "      <td>[it, doesn, t, matter, whoever, won, between, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>nognog nognog nag malasakit wtf tangina mo bin...</td>\n",
       "      <td>[nognog, nognog, nag, malasakit, wtf, tangina,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>what abi binay said on running for makati mayor</td>\n",
       "      <td>[what, abi, binay, said, on, running, for, mak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>srsly how can binay do away with no tax for th...</td>\n",
       "      <td>[srsly, how, can, binay, do, away, with, no, t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          rm_stpwrds  \\\n",
       "0       inaasahan vice president jejomar binay taong   \n",
       "1  mar roxas tang ina tuwid daan daw eh sya nga d...   \n",
       "2  salamat sawang suporta taga makati pagbabalik ...   \n",
       "3                           putangina mo binay takbo   \n",
       "4  binay with selective amnesia forgetting about ...   \n",
       "5  it doesn t matter whoever won between duterte ...   \n",
       "6  nognog nognog nag malasakit wtf tangina mo bin...   \n",
       "7                                                      \n",
       "8    what abi binay said on running for makati mayor   \n",
       "9  srsly how can binay do away with no tax for th...   \n",
       "\n",
       "                                            tokenize  \n",
       "0  [inaasahan, vice, president, jejomar, binay, t...  \n",
       "1  [mar, roxas, tang, ina, tuwid, daan, daw, eh, ...  \n",
       "2  [salamat, sawang, suporta, taga, makati, pagba...  \n",
       "3                      [putangina, mo, binay, takbo]  \n",
       "4  [binay, with, selective, amnesia, forgetting, ...  \n",
       "5  [it, doesn, t, matter, whoever, won, between, ...  \n",
       "6  [nognog, nognog, nag, malasakit, wtf, tangina,...  \n",
       "7                                                 []  \n",
       "8  [what, abi, binay, said, on, running, for, mak...  \n",
       "9  [srsly, how, can, binay, do, away, with, no, t...  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[['rm_stpwrds','tokenize']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokenize</th>\n",
       "      <th>lematize</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>[haha, binay, jollibee]</td>\n",
       "      <td>[haha, binay, jollibee]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[si, senator, nancy, binay, ba, tinutukoy, mo,...</td>\n",
       "      <td>[si, senator, nancy, binay, ba, tinutukoy, mo,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>[duterte, roxas, poe, binay, santiago, rank]</td>\n",
       "      <td>[duterte, roxas, poe, binay, santiago, rank]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[putangina, mo, binay, takbo]</td>\n",
       "      <td>[putangina, mo, binay, takbo]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>[say, goodbye, to, your, presidential, ambitio...</td>\n",
       "      <td>[say, goodbye, to, your, presidential, ambitio...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             tokenize  \\\n",
       "31                            [haha, binay, jollibee]   \n",
       "21  [si, senator, nancy, binay, ba, tinutukoy, mo,...   \n",
       "70       [duterte, roxas, poe, binay, santiago, rank]   \n",
       "3                       [putangina, mo, binay, takbo]   \n",
       "78  [say, goodbye, to, your, presidential, ambitio...   \n",
       "\n",
       "                                             lematize  \n",
       "31                            [haha, binay, jollibee]  \n",
       "21  [si, senator, nancy, binay, ba, tinutukoy, mo,...  \n",
       "70       [duterte, roxas, poe, binay, santiago, rank]  \n",
       "3                       [putangina, mo, binay, takbo]  \n",
       "78  [say, goodbye, to, your, presidential, ambitio...  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "def lema_words(text):\n",
    "  wnl=WordNetLemmatizer()\n",
    "  return[wnl.lemmatize(w) for w in text]\n",
    "\n",
    "train['lematize']=train['tokenize'].apply(lema_words)  \n",
    "train[['tokenize','lematize']].sample(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proceeding to Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Train the SVM model\u001b[39;00m\n\u001b[0;32m      2\u001b[0m clf \u001b[39m=\u001b[39m svm\u001b[39m.\u001b[39mSVC()\n\u001b[1;32m----> 3\u001b[0m clf\u001b[39m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m      5\u001b[0m \u001b[39m# Test the model on the testing set\u001b[39;00m\n\u001b[0;32m      6\u001b[0m y_pred \u001b[39m=\u001b[39m clf\u001b[39m.\u001b[39mpredict(X_test)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "# Train the SVM model\n",
    "clf = svm.SVC()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the testing set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "accuracy = clf.score(X_test, y_test)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training with lemmatized words\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(train['lematize'], train['label'], test_size=0.3, random_state=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the sentences using TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(map(' '.join, X_train))\n",
    "X_test = vectorizer.transform(map(' '.join, X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7242105263157895\n"
     ]
    }
   ],
   "source": [
    "# Train the SVM model\n",
    "clf = svm.SVC()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the testing set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "accuracy = clf.score(X_test, y_test)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOWELS = \"aeiouAEIOU\"\n",
    "CONSONANTS = \"bcdfghklmnngpqrstvwyBCDFGHKLMNNGPQRSTVWY\"\n",
    "\n",
    "\"\"\" \n",
    "\tAffixes\n",
    "\"\"\"\n",
    "PREFIX_SET = [\n",
    "\t'nakikipag', 'pakikipag',\n",
    "\t'pinakama', 'pagpapa',\n",
    "\t'pinagka', 'panganga', \n",
    "\t'makapag', 'nakapag', \n",
    "\t'tagapag', 'makipag', \n",
    "\t'nakipag', 'tigapag',\n",
    "\t'pakiki', 'magpa',\n",
    "\t'napaka', 'pinaka',\n",
    "\t'ipinag', 'pagka', \n",
    "\t'pinag', 'mapag', \n",
    "\t'mapa', 'taga', \n",
    "\t'ipag', 'tiga', \n",
    "\t'pala', 'pina', \n",
    "\t'pang', 'naka',\n",
    "\t'nang', 'mang',\n",
    "\t'sing',\n",
    "\t'ipa', 'pam',\n",
    "\t'pan', 'pag',\n",
    "\t'tag', 'mai',\n",
    "\t'mag', 'nam',\n",
    "\t'nag', 'man',\n",
    "\t'may', 'ma',\n",
    "\t'na', 'ni',\n",
    "\t'pa', 'ka',\n",
    "\t'um', 'in',\n",
    "\t'i',\n",
    "]\n",
    "\n",
    "INFIX_SET = [\n",
    "\t'um', 'in',\n",
    "]\n",
    "\n",
    "SUFFIX_SET = [\n",
    "\t'syon','dor', \n",
    "\t'ita', 'han', \n",
    "\t'hin', 'ing', \n",
    "\t'ang', 'ng', \n",
    "\t'an', 'in', \n",
    "\t'g',\n",
    "]\n",
    "\n",
    "PERIOD_FLAG = True\n",
    "PASS_FLAG = False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def check_vowel(substring):\n",
    "\t\"\"\"\n",
    "\t\tChecks if the substring is a vowel.\n",
    "\t\t\tletters: substring to be tested\n",
    "\t\treturns BOOLEAN\n",
    "\t\"\"\"\n",
    "\n",
    "\treturn all(letter in VOWELS for letter in substring)\n",
    "\n",
    "\n",
    "def check_consonant(substring):\n",
    "\t\"\"\"\n",
    "\t\tChecks if the letter is a consonant.\n",
    "\t\t\tletter: substring to be tested\n",
    "\t\treturns BOOLEAN\n",
    "\t\"\"\"\n",
    "\n",
    "\treturn all(letter in CONSONANTS for letter in substring)\n",
    "\n",
    "def change_letter(token, index, letter):\n",
    "\t\"\"\"\n",
    "\t\tReplaces a letter in a token.\n",
    "\t\t\ttoken: word to be used\n",
    "\t\t\tindex: index of the letter\n",
    "\t\t\tletter: letter used to replace\n",
    "\t\treturns STRING\n",
    "\t\"\"\"\n",
    "\t\n",
    "\t_list = list(token)\n",
    "\t_list[index] = letter\n",
    "\n",
    "\treturn ''.join(_list)\n",
    "\n",
    "def count_vowel(token):\n",
    "\t\"\"\"\n",
    "\t\tCount vowels in a given token.\n",
    "\t\t\ttoken: string to be counted for vowels\n",
    "\t\treturns INTEGER\n",
    "\t\"\"\"\n",
    "\n",
    "\tcount = 0\n",
    "\n",
    "\tfor tok in token:\n",
    "\t\tif check_vowel(tok):\n",
    "\t\t\tcount+=1\n",
    "\n",
    "\treturn count\n",
    "\n",
    "\n",
    "def count_consonant(token):\n",
    "\t\"\"\"\n",
    "\t\tCount consonants in a given token.\n",
    "\t\t\ttoken: string to be counted for consonants\n",
    "\t\treturns INTEGER\n",
    "\t\"\"\"\n",
    "\n",
    "\tcount = 0\n",
    "\n",
    "\tfor tok in token:\n",
    "\t\tif check_consonant(tok):\n",
    "\t\t\tcount+=1\n",
    "\n",
    "\treturn count\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def check_validation(token):\n",
    "    with open('stemmer/validation.txt', 'r') as valid:\n",
    "        data = valid.read().replace('\\n', ' ').split(' ')\n",
    "\n",
    "    return token in data\n",
    "\n",
    "\n",
    "\n",
    "def clean_repitition(token, REPITITION):\n",
    "\t\"\"\"\n",
    "\t\tChecks token for repitition. (ex. nakakabaliw = nabaliw)\n",
    "\t\t\ttoken: word to be stemmed repitition\n",
    "\t\treturns STRING\n",
    "\t\"\"\"\n",
    "\n",
    "\tif check_validation(token):\n",
    "\t\treturn token\n",
    "\n",
    "\tif len(token) >= 4:\n",
    "\t\tif check_vowel(token[0]):\n",
    "\t\t\tif token[0] == token[1]:\n",
    "\t\t\t\tREPITITION.append(token[0])\n",
    "\t\t\t\treturn token[1:]\n",
    "\n",
    "\t\telif check_consonant(token[0]) and count_vowel(token) >= 2:\n",
    "\t\t\tif token[0: 2] == token[2: 4] and len(token) - 2 >= 4:\n",
    "\t\t\t\tREPITITION.append(token[2:4])\n",
    "\t\t\t\treturn token[2:]\n",
    "\t\t\t\n",
    "\t\t\telif token[0: 3] == token[3: 6] and len(token) - 3 >= 4:\n",
    "\t\t\t\tREPITITION.append(token[3:6])\n",
    "\t\t\t\treturn token[3:]\n",
    "\n",
    "\treturn token\n",
    "\n",
    "def clean_suffix(token, SUFFIX):\n",
    "    \"\"\"\n",
    "    Checks token for suffixes. (ex. bigayan = bigay)\n",
    "        token: word to be stemmed for suffixes\n",
    "    returns STRING\n",
    "    \"\"\"\n",
    "\n",
    "    SUF_CANDIDATE = []\n",
    "\n",
    "    if check_validation(token):\n",
    "        return token\n",
    "\n",
    "    for suffix in SUFFIX_SET:\n",
    "        if len(token) - len(suffix) >= 3 and count_vowel(token[0:len(token) - len(suffix)]) >= 2 and count_consonant(token[0:len(token) - len(suffix)]) >= 1:\n",
    "            if token[len(token) - len(suffix): len(token)] == suffix:\n",
    "                if len(suffix) == 2 and not count_consonant(token[0:len(token) - len(suffix)]) >= 1:\n",
    "                    continue\n",
    "\n",
    "                if count_vowel(token[0: len(token) - len(suffix)]) >= 2:\n",
    "                    if suffix == 'ang' and check_consonant(token[-4]) \\\n",
    "                            and token[-4] != 'r' and token[-5] != 'u':\n",
    "                        continue\n",
    "\n",
    "                    #print(token[0: len(token) - len(suffix)] + \" : \" + suffix)\n",
    "\n",
    "                    if check_validation(token[0: len(token) - len(suffix)]):\n",
    "                        SUFFIX.append(suffix)\n",
    "                        return token[0: len(token) - len(suffix)] + 'a' if suffix == 'ita' \\\n",
    "                            else token[0: len(token) - len(suffix)]\n",
    "\n",
    "                    elif len(SUF_CANDIDATE) == 0:\n",
    "                        SUF_CANDIDATE.append(suffix)\n",
    "                        SUF_CANDIDATE.append(token[0: len(token) - len(suffix)])\n",
    "\n",
    "    if (len(SUF_CANDIDATE) == 2):\n",
    "        SUFFIX = SUF_CANDIDATE[0]\n",
    "        return SUF_CANDIDATE[1][0: len(token) - len(SUFFIX)] + 'a' if SUFFIX == 'ita' \\\n",
    "            else SUF_CANDIDATE[1][0: len(token) - len(SUFFIX)]\n",
    "\n",
    "    return token\n",
    "\n",
    "\n",
    "def clean_infix(token, INFIX):\n",
    "\t\"\"\"\n",
    "\t\tChecks token for infixes. (ex. bumalik = balik)\n",
    "\t\t\ttoken: word to be stemmed for infixes\n",
    "\t\treturns STRING\n",
    "\t\"\"\"\n",
    "\n",
    "\tif check_validation(token):\n",
    "\t\treturn token\n",
    "\n",
    "\tfor infix in INFIX_SET:\n",
    "\t\tif len(token) - len(infix) >= 3 and count_vowel(token[len(infix):]) >= 2:\n",
    "\t\t\tif token[0] == token[4] and token[1: 4] == infix:\n",
    "\t\t\t\tINFIX.append(infix)\n",
    "\t\t\t\treturn token[4:]\n",
    "\n",
    "\t\t\telif token[2] == token[4] and token[1: 3] == infix:\n",
    "\t\t\t\tINFIX.append(infix)\n",
    "\t\t\t\treturn token[0] + token[3:]\n",
    "\n",
    "\t\t\telif token[1: 3] == infix and check_vowel(token[3]):\n",
    "\t\t\t\tINFIX.append(infix)\n",
    "\t\t\t\treturn token[0] + token[3:]\n",
    "\n",
    "\treturn token\n",
    "\n",
    "\n",
    "def clean_prefix(token,\t PREFIX):\n",
    "\t\"\"\"\n",
    "\t\tChecks token for prefixes. (ex. naligo = ligo)\n",
    "\t\t\ttoken: word to be stemmed for prefixes\n",
    "\t\treturns STRING\n",
    "\t\"\"\"\n",
    "\n",
    "\tif check_validation(token):\n",
    "\t\treturn token\n",
    "\n",
    "\tfor prefix in PREFIX_SET:\n",
    "\t\tif len(token) - len(prefix) >= 3 and \\\n",
    "\t\t\tcount_vowel(token[len(prefix):]) >= 2:\n",
    "\n",
    "\t\t\tif prefix == ('i') and check_consonant(token[2]):\n",
    "\t\t\t\tcontinue\n",
    "\n",
    "\t\t\tif '-' in token:\t\n",
    "\t\t\t\ttoken = token.split('-')\n",
    "\n",
    "\t\t\t\tif token[0] == prefix and check_vowel(token[1][0]):\n",
    "\t\t\t\t\tPREFIX.append(prefix)\n",
    "\t\t\t\t\treturn token[1]\n",
    "\n",
    "\t\t\t\ttoken = '-'.join(token)\n",
    "\n",
    "\t\t\tif token[0: len(prefix)] == prefix:\n",
    "\t\t\t\tif count_vowel(token[len(prefix):]) >= 2:\n",
    "\t\t\t\t\t# if check_vowel(token[len(token) - len(prefix) - 1]):\n",
    "\t\t\t\t# \tcontinue\n",
    "\n",
    "\t\t\t\t\tif prefix == 'panganga':\n",
    "\t\t\t\t\t\tPREFIX.append(prefix)\n",
    "\t\t\t\t\t\treturn 'ka' + token[len(prefix):]\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\tPREFIX.append(prefix)\n",
    "\t\t\t\t\treturn token[len(prefix):]\n",
    "\n",
    "\treturn token\n",
    "\n",
    "\n",
    "def clean_duplication(token, DUPLICATE):\n",
    "\t\"\"\"\n",
    "\t\tChecks token for duplication. (ex. araw-araw = araw)\n",
    "\t\t\ttoken: word to be stemmed duplication\n",
    "\t\treturns STRING\n",
    "\t\"\"\"\n",
    "\n",
    "\tif check_validation(token):\n",
    "\t\treturn token\n",
    "\n",
    "\tif '-' in token and token.index('-') != 0 and \\\n",
    "\t\ttoken.index('-') != len(token) -  1:\n",
    "\n",
    "\t\tsplit = token.split('-')\n",
    "\n",
    "\t\tif all(len(tok) >= 3 for tok in split):\n",
    "\t\t\tif split[0] == token[1] or split[0][-1] == 'u' and change_letter(split[0], -1, 'o') == split[1] or \\\n",
    "\t\t\t\tsplit[0][-2] == 'u' and change_letter(split[0], -2, 'o')  == split[1]:\n",
    "\t\t\t\tDUPLICATE.append(split[0])\n",
    "\t\t\t\treturn split[0]\n",
    "\n",
    "\t\t\telif split[0] == split[1][0:len(split[0])]:\n",
    "\t\t\t\tDUPLICATE.append(split[1])\n",
    "\t\t\t\treturn split[1]\n",
    "\n",
    "\t\t\telif split[0][-2:] == 'ng':\n",
    "\t\t\t\tif split[0][-3] == 'u':\n",
    "\t\t\t\t\tif split[0][0:-3] + 'o' == split[1]:\n",
    "\t\t\t\t\t\tDUPLICATE.append(split[1])\n",
    "\t\t\t\t\t\treturn split[1]\n",
    "\n",
    "\t\t\t\tif split[0][0:-2] == split[1]:\n",
    "\t\t\t\t\tDUPLICATE.append(split[1])\n",
    "\t\t\t\t\treturn split[1]\n",
    "\n",
    "\t\telse:\n",
    "\t\t\treturn '-'.join(split)\n",
    "\t\n",
    "\treturn token\n",
    "\n",
    "\n",
    "def clean_repitition(token, REPITITION):\n",
    "\t\"\"\"\n",
    "\t\tChecks token for repitition. (ex. nakakabaliw = nabaliw)\n",
    "\t\t\ttoken: word to be stemmed repitition\n",
    "\t\treturns STRING\n",
    "\t\"\"\"\n",
    "\n",
    "\tif check_validation(token):\n",
    "\t\treturn token\n",
    "\n",
    "\tif len(token) >= 4:\n",
    "\t\tif check_vowel(token[0]):\n",
    "\t\t\tif token[0] == token[1]:\n",
    "\t\t\t\tREPITITION.append(token[0])\n",
    "\t\t\t\treturn token[1:]\n",
    "\n",
    "\t\telif check_consonant(token[0]) and count_vowel(token) >= 2:\n",
    "\t\t\tif token[0: 2] == token[2: 4] and len(token) - 2 >= 4:\n",
    "\t\t\t\tREPITITION.append(token[2:4])\n",
    "\t\t\t\treturn token[2:]\n",
    "\t\t\t\n",
    "\t\t\telif token[0: 3] == token[3: 6] and len(token) - 3 >= 4:\n",
    "\t\t\t\tREPITITION.append(token[3:6])\n",
    "\t\t\t\treturn token[3:]\n",
    "\n",
    "\treturn token\n",
    "\n",
    "\n",
    "def clean_stemmed(token, CLEANERS, REPITITION):\n",
    "\t\t\n",
    "\tif not token:\n",
    "\t\treturn \"\"\n",
    "\t\n",
    "\t\"\"\"\n",
    "\t\tChecks for left-over affixes and letters.\n",
    "\t\t\ttoken: word to be cleaned for excess affixes/letters\n",
    "\t\treturns STRING\n",
    "\t\"\"\"\n",
    "\n",
    "\tglobal PERIOD_FLAG\n",
    "\tglobal PASS_FLAG\n",
    "\n",
    "\tCC_EXP = ['dr', 'gl', 'gr', 'ng', 'kr', 'kl', 'kw', 'ts', 'tr', 'pr', 'pl', 'pw', 'sw', 'sy'] # Consonant + Consonant Exceptions\n",
    "\n",
    "\tif token[-1] == '.' and PASS_FLAG == False:\n",
    "\t\tPERIOD_FLAG = True\n",
    "\n",
    "\tif not check_vowel(token[-1]) and not check_consonant(token[-1]):\n",
    "\t\tCLEANERS.append(token[-1])\n",
    "\t\ttoken = token[0:-1]\n",
    "\n",
    "#\tif not check_vowel(token[0]) and not check_consonant(token[0]):\n",
    "#\t\tCLEANERS.append(token[0])\n",
    "#\t\ttoken = token[1:]\n",
    "\n",
    "\tif check_validation(token):\n",
    "\t\treturn token\n",
    "\n",
    "\tif len(token) >= 3 and count_vowel(token) >= 2:\n",
    "\t\ttoken = clean_repitition(token,\tREPITITION)\n",
    "\n",
    "\t\tif check_consonant(token[-1]) and token[- 2] == 'u':\n",
    "\t\t\tCLEANERS.append('u')\n",
    "\t\t\ttoken = change_letter(token, -2, 'o')\n",
    "\n",
    "\t\tif token[len(token) - 1] == 'u':\n",
    "\t\t\tCLEANERS.append('u')\n",
    "\t\t\ttoken = change_letter(token, -1, 'o')\n",
    "\n",
    "\t\tif token[-1] == 'r':\n",
    "\t\t\tCLEANERS.append('r')\n",
    "\t\t\ttoken = change_letter(token, -1, 'd')\n",
    "\n",
    "\t\tif token[-1] == 'h' and check_vowel(token[-1]):\n",
    "\t\t\tCLEANERS.append('h')\n",
    "\t\t\ttoken = token[0:-1]\n",
    "\n",
    "\t\t# if token[0] == 'i':\n",
    "\t\t# \ttoken = token[1:]\n",
    "\n",
    "\t\tif token[0] == token[1]:\n",
    "\t\t\tCLEANERS.append(token[0])\n",
    "\t\t\ttoken = token[1:]\n",
    "\n",
    "\t\tif (token[0: 2] == 'ka' or token[0: 2] == 'pa') and check_consonant(token[2]) \\\n",
    "\t\t\tand count_vowel(token) >= 3:\n",
    "\t\t\t\n",
    "\t\t\tCLEANERS.append(token[0: 2])\n",
    "\t\t\ttoken = token[2:]\n",
    "\n",
    "\t\tif(token[-3:]) == 'han' and count_vowel(token[0:-3]) == 1:\n",
    "\t\t\tCLEANERS.append('han')\n",
    "\t\t\ttoken = token[0:-3] + 'i'\n",
    "\n",
    "\t\tif(token[-3:]) == 'han' and count_vowel(token[0:-3]) > 1:\n",
    "\t\t\tCLEANERS.append('han')\n",
    "\t\t\ttoken = token[0:-3]\n",
    "\n",
    "\t\tif len(token) >= 2 and count_vowel(token) >= 3:\n",
    "\t\t\tif token[-1] == 'h' and check_vowel(token[-2]):\n",
    "\t\t\t\tCLEANERS.append('h')\n",
    "\t\t\t\ttoken = token[0:-1]\n",
    "\n",
    "\t\tif len(token) >= 6 and token[0:2] == token[2:4]:\n",
    "\t\t\tCLEANERS.append('0:2')\n",
    "\t\t\ttoken = token[2:]\n",
    "\n",
    "\t\tif any(REP[0] == 'r' for REP in REPITITION):\n",
    "\t\t\tCLEANERS.append('r')\n",
    "\t\t\ttoken = change_letter(token, 0, 'd')\n",
    "\n",
    "\t\tif token[-2:] == 'ng' and token[-3] == 'u':\n",
    "\t\t\tCLEANERS.append('u')\n",
    "\t\t\ttoken = change_letter(token, -3, 'o')\n",
    "\n",
    "\t\tif token[-1] == 'h':\n",
    "\t\t\tCLEANERS.append('h')\n",
    "\t\t\ttoken = token[0:-1]\n",
    "\n",
    "\t\tif any(token[0:2] != CC for CC in CC_EXP) and check_consonant(token[0:2]):\n",
    "\t\t\tCLEANERS.append(token[0:2])\n",
    "\t\t\ttoken = token[1:]\n",
    "\n",
    "\treturn token\n",
    "\n",
    "\n",
    "def tg_stemmer(tokens):\n",
    "\n",
    "    global PERIOD_FLAG\n",
    "    global PASS_FLAG\n",
    "\n",
    "    pre_stem     = inf_stem = suf_stem = rep_stem = \\\n",
    "        du1_stem = du2_stem = cle_stem = '-'\n",
    "    word_info    = {}\n",
    "    PREFIX     = []\n",
    "    INFIX      = []\n",
    "    SUFFIX     = []\n",
    "    DUPLICATE  = []\n",
    "    REPITITION = []\n",
    "    CLEANERS   = []\n",
    "\n",
    "    word_info['clean'] = '-'\n",
    "    stemmed_tokens = []\n",
    "\n",
    "    for token in tokens:\n",
    "        word_info = {}\n",
    "        word_info[\"word\"] = token\n",
    "\n",
    "        if (PERIOD_FLAG == True and token[0].isupper()) or \\\n",
    "                (PERIOD_FLAG == False and token[0].islower()):\n",
    "            token = token.lower()\n",
    "            du1_stem = clean_duplication(token, DUPLICATE)\n",
    "            pre_stem = clean_prefix(du1_stem, PREFIX)\n",
    "            rep_stem = clean_repitition(pre_stem, REPITITION)\n",
    "            inf_stem = clean_infix(rep_stem, INFIX)\n",
    "            rep_stem = clean_repitition(inf_stem, REPITITION)\n",
    "            suf_stem = clean_suffix(rep_stem, SUFFIX)\n",
    "            du2_stem = clean_duplication(suf_stem, DUPLICATE)\n",
    "            cle_stem = clean_stemmed(du2_stem, CLEANERS, REPITITION)\n",
    "            cle_stem = clean_duplication(cle_stem, DUPLICATE)\n",
    "\n",
    "            if '-' in cle_stem:\n",
    "                cle_stem.replace('-', '')\n",
    "\n",
    "        else:\n",
    "            PERIOD_FLAG = False\n",
    "            cle_stem = clean_stemmed(token, CLEANERS, REPITITION)\n",
    "            word_info[\"root\"]   = token\n",
    "            word_info[\"prefix\"] = '[]'\n",
    "            word_info[\"infix\"]  = '[]'\n",
    "            word_info[\"suffix\"] = '[]'\n",
    "            word_info[\"repeat\"] = '[]'\n",
    "            word_info[\"dupli\"]  = '[]'\n",
    "            word_info[\"clean\"]  = cle_stem\n",
    "\n",
    "        stemmed_tokens.append(cle_stem)\n",
    "\n",
    "    return stemmed_tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ENGLISH STEMMER\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def english_lemmatizer(token):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return lemmatizer.lemmatize(token)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'appendstr'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[67], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     19\u001b[0m             lemmatized_token \u001b[39m=\u001b[39m token\n\u001b[1;32m---> 20\u001b[0m         lemmatized_row\u001b[39m.\u001b[39;49mappendstr(lemmatized_token)\n\u001b[0;32m     21\u001b[0m     lemmatized_tokens\u001b[39m.\u001b[39mappend(\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(lemmatized_row))\n\u001b[0;32m     23\u001b[0m train[\u001b[39m'\u001b[39m\u001b[39mlemmatize_modstem\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mSeries(lemmatized_tokens)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'appendstr'"
     ]
    }
   ],
   "source": [
    "from langdetect import detect\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "lemmatized_tokens = []\n",
    "for row in train['tokenize']:\n",
    "    lemmatized_row = []\n",
    "    for token in row:\n",
    "        try: \n",
    "            language = detect(token)\n",
    "        except:\n",
    "            continue\n",
    "        if language == 'tl':\n",
    "            lemmatized_token = tg_stemmer(token)\n",
    "        elif language == 'en':\n",
    "            lemmatized_token = english_lemmatizer(token)\n",
    "        else:\n",
    "            lemmatized_token = token\n",
    "        lemmatized_row.appendstr(lemmatized_token)\n",
    "    lemmatized_tokens.append(' '.join(lemmatized_row))\n",
    "\n",
    "train['lemmatize_modstem'] = pd.Series(lemmatized_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tokenize</th>\n",
       "      <th>lemmatize_modstem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[inaasahan, vice, president, jejomar, binay, t...</td>\n",
       "      <td>inaasahan vice president jejomar y g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[mar, roxas, tang, ina, tuwid, daan, daw, eh, ...</td>\n",
       "      <td>mar roxas g ina tuwid daan daw eh a a di straight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>[salamat, sawang, suporta, taga, makati, pagba...</td>\n",
       "      <td>t g suporta a makati k y in makati</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>[putangina, mo, binay, takbo]</td>\n",
       "      <td>a mo y takbo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>[binay, with, selective, amnesia, forgetting, ...</td>\n",
       "      <td>y with selective amnesia forgetting about the ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                           tokenize  \\\n",
       "0      0  [inaasahan, vice, president, jejomar, binay, t...   \n",
       "1      1  [mar, roxas, tang, ina, tuwid, daan, daw, eh, ...   \n",
       "2      0  [salamat, sawang, suporta, taga, makati, pagba...   \n",
       "3      1                      [putangina, mo, binay, takbo]   \n",
       "4      0  [binay, with, selective, amnesia, forgetting, ...   \n",
       "\n",
       "                                   lemmatize_modstem  \n",
       "0               inaasahan vice president jejomar y g  \n",
       "1  mar roxas g ina tuwid daan daw eh a a di straight  \n",
       "2                 t g suporta a makati k y in makati  \n",
       "3                                       a mo y takbo  \n",
       "4  y with selective amnesia forgetting about the ...  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[['label','tokenize','lemmatize_modstem']].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object, got 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[68], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnltk\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtokenize\u001b[39;00m \u001b[39mimport\u001b[39;00m WhitespaceTokenizer\n\u001b[1;32m----> 5\u001b[0m train[\u001b[39m'\u001b[39m\u001b[39mlemmatize_modstem\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m train[\u001b[39m'\u001b[39;49m\u001b[39mlemmatize_modstem\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mapply(nltk\u001b[39m.\u001b[39;49mtokenize\u001b[39m.\u001b[39;49mWhitespaceTokenizer()\u001b[39m.\u001b[39;49mtokenize) \n\u001b[0;32m      6\u001b[0m train[\u001b[39m'\u001b[39m\u001b[39mlemmatize_modstem\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mhead(\u001b[39m10\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\series.py:4771\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4661\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[0;32m   4662\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   4663\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4666\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   4667\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[0;32m   4668\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   4669\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4670\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4769\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4770\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4771\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\u001b[39mself\u001b[39;49m, func, convert_dtype, args, kwargs)\u001b[39m.\u001b[39;49mapply()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\apply.py:1123\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1120\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_str()\n\u001b[0;32m   1122\u001b[0m \u001b[39m# self.f is Callable\u001b[39;00m\n\u001b[1;32m-> 1123\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\apply.py:1174\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1172\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1173\u001b[0m         values \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m)\u001b[39m.\u001b[39m_values\n\u001b[1;32m-> 1174\u001b[0m         mapped \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49mmap_infer(\n\u001b[0;32m   1175\u001b[0m             values,\n\u001b[0;32m   1176\u001b[0m             f,\n\u001b[0;32m   1177\u001b[0m             convert\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_dtype,\n\u001b[0;32m   1178\u001b[0m         )\n\u001b[0;32m   1180\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1181\u001b[0m     \u001b[39m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1182\u001b[0m     \u001b[39m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1183\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39m_constructor_expanddim(\u001b[39mlist\u001b[39m(mapped), index\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\_libs\\lib.pyx:2924\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\nltk\\tokenize\\regexp.py:127\u001b[0m, in \u001b[0;36mRegexpTokenizer.tokenize\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gaps:\n\u001b[0;32m    126\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_discard_empty:\n\u001b[1;32m--> 127\u001b[0m         \u001b[39mreturn\u001b[39;00m [tok \u001b[39mfor\u001b[39;00m tok \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_regexp\u001b[39m.\u001b[39msplit(text) \u001b[39mif\u001b[39;00m tok]\n\u001b[0;32m    128\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    129\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_regexp\u001b[39m.\u001b[39msplit(text)\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object, got 'list'"
     ]
    }
   ],
   "source": [
    "#tokenization \n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "train['lemmatize_modstem'] = train['lemmatize_modstem'].apply(nltk.tokenize.WhitespaceTokenizer().tokenize) \n",
    "train['lemmatize_modstem'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(train['lemmatize_modstem'], train['label'], test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the sentences using TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(map(' '.join, X_train))\n",
    "X_test = vectorizer.transform(map(' '.join, X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7249122807017544\n"
     ]
    }
   ],
   "source": [
    "# Train the SVM model\n",
    "clf = svm.SVC()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the testing set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "accuracy = clf.score(X_test, y_test)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
