{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import neccessary dependencies\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import svm\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import joblib\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data\n",
    "train = pd.read_csv(r\"C:\\Users\\Admin\\OneDrive\\Documents\\Webdev\\hatespeech\\train.csv\", nrows=5000)\n",
    "\n",
    "# testing data\n",
    "test = pd.read_csv(r\"C:\\Users\\Admin\\OneDrive\\Documents\\Webdev\\hatespeech\\test.csv\", nrows=5000)\n",
    "\n",
    "\n",
    "# # # #training data\n",
    "# train = pd.read_csv(r\"C:\\Users\\jmest\\Documents\\Files\\C4S2\\Thesis\\ThesisFiles-main\\ThesisFiles-main\\hatespeech\\train.csv\", nrows=5000)\n",
    "\n",
    "# # # #testing data\n",
    "# test = pd.read_csv(r\"C:\\Users\\jmest\\Documents\\Files\\C4S2\\Thesis\\ThesisFiles-main\\ThesisFiles-main\\hatespeech\\test.csv\", nrows=5000)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean the data of null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Inaasahan na ni Vice President Jejomar Binay n...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mar Roxas TANG INA TUWID NA DAAN DAW .. EH SYA...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Salamat sa walang sawang suporta ng mga taga m...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@rapplerdotcom putangina mo binay TAKBO PA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Binay with selective amnesia, forgetting about...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>Awww DUTERTE Na wag Lang si Roxas</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>RT @mikkieugenio: If the SC disqualifies Poe a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>\"Pag naging presidente si Binay, wala kayong t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>Yan na naman ang walang kwentang commercial ni...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>https://t.co/CeNBKhrm8P</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  label\n",
       "0     Inaasahan na ni Vice President Jejomar Binay n...      0\n",
       "1     Mar Roxas TANG INA TUWID NA DAAN DAW .. EH SYA...      1\n",
       "2     Salamat sa walang sawang suporta ng mga taga m...      0\n",
       "3            @rapplerdotcom putangina mo binay TAKBO PA      1\n",
       "4     Binay with selective amnesia, forgetting about...      0\n",
       "...                                                 ...    ...\n",
       "4995                  Awww DUTERTE Na wag Lang si Roxas      0\n",
       "4996  RT @mikkieugenio: If the SC disqualifies Poe a...      0\n",
       "4997  \"Pag naging presidente si Binay, wala kayong t...      0\n",
       "4998  Yan na naman ang walang kwentang commercial ni...      1\n",
       "4999                            https://t.co/CeNBKhrm8P      0\n",
       "\n",
       "[5000 rows x 2 columns]"
      ]
     },
     "execution_count": 527,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop rows with null values\n",
    "train.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text     0\n",
       "label    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 528,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check for null values in train\n",
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2653"
      ]
     },
     "execution_count": 529,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for 0 values in train\n",
    "sum(train[\"label\"] == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2347"
      ]
     },
     "execution_count": 530,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for 1 values in train\n",
    "sum(train[\"label\"] == 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean the data of unwated Text and Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove special characters using the regular expression library\n",
    "\n",
    "import re\n",
    "\n",
    "#set up punctuations we want to be replaced\n",
    "\n",
    "REPLACE_NO_SPACE = re.compile(\"(\\.)|(\\;)|(\\:)|(\\!)|(\\')|(\\?)|(\\,)|(\\\")|(\\|)|(\\()|(\\))|(\\[)|(\\])|(\\%)|(\\$)|(\\>)|(\\<)|(\\{)|(\\})\")\n",
    "REPLACE_WITH_SPACE = re.compile(\"(<br\\s/><br\\s/?)|(-)|(/)|(:).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [],
   "source": [
    "import preprocessor as p\n",
    "\n",
    "# custum function to clean the dataset (combining tweet_preprocessor and reguar expression)\n",
    "def clean_tweets(df):\n",
    "    tempArr = []\n",
    "    for line in df:\n",
    "        # send to tweet_processor\n",
    "        tmpL = p.clean(line)\n",
    "        # remove everything except letters and digits\n",
    "        tmpL = re.sub(r'[^a-zA-Z0-9\\s]', ' ', tmpL)\n",
    "        # convert to lowercase\n",
    "        tmpL = tmpL.lower()\n",
    "        tempArr.append(tmpL)\n",
    "    return tempArr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean training data\n",
    "train_tweet = clean_tweets(train[\"text\"])\n",
    "train_tweet = pd.DataFrame(train_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>clean_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Inaasahan na ni Vice President Jejomar Binay n...</td>\n",
       "      <td>0</td>\n",
       "      <td>inaasahan na ni vice president jejomar binay n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mar Roxas TANG INA TUWID NA DAAN DAW .. EH SYA...</td>\n",
       "      <td>1</td>\n",
       "      <td>mar roxas tang ina tuwid na daan daw    eh sya...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Salamat sa walang sawang suporta ng mga taga m...</td>\n",
       "      <td>0</td>\n",
       "      <td>salamat sa walang sawang suporta ng mga taga m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@rapplerdotcom putangina mo binay TAKBO PA</td>\n",
       "      <td>1</td>\n",
       "      <td>putangina mo binay takbo pa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Binay with selective amnesia, forgetting about...</td>\n",
       "      <td>0</td>\n",
       "      <td>binay with selective amnesia  forgetting about...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>It doesn't matter whoever won between Duterte ...</td>\n",
       "      <td>0</td>\n",
       "      <td>it doesn t matter whoever won between duterte ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Nognog? Pero nognog din ang nag malasakit? Wtf...</td>\n",
       "      <td>1</td>\n",
       "      <td>nognog  pero nognog din ang nag malasakit  wtf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>#OnlyB1nay ?? #FB https://t.co/QEQnsK67Gm</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>What Abi Binay said on running for Makati mayo...</td>\n",
       "      <td>0</td>\n",
       "      <td>what abi binay said on running for makati mayor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Srsly. How can Binay do away with no tax for t...</td>\n",
       "      <td>1</td>\n",
       "      <td>srsly  how can binay do away with no tax for t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label   \n",
       "0  Inaasahan na ni Vice President Jejomar Binay n...      0  \\\n",
       "1  Mar Roxas TANG INA TUWID NA DAAN DAW .. EH SYA...      1   \n",
       "2  Salamat sa walang sawang suporta ng mga taga m...      0   \n",
       "3         @rapplerdotcom putangina mo binay TAKBO PA      1   \n",
       "4  Binay with selective amnesia, forgetting about...      0   \n",
       "5  It doesn't matter whoever won between Duterte ...      0   \n",
       "6  Nognog? Pero nognog din ang nag malasakit? Wtf...      1   \n",
       "7          #OnlyB1nay ?? #FB https://t.co/QEQnsK67Gm      1   \n",
       "8  What Abi Binay said on running for Makati mayo...      0   \n",
       "9  Srsly. How can Binay do away with no tax for t...      1   \n",
       "\n",
       "                                         clean_tweet  \n",
       "0  inaasahan na ni vice president jejomar binay n...  \n",
       "1  mar roxas tang ina tuwid na daan daw    eh sya...  \n",
       "2  salamat sa walang sawang suporta ng mga taga m...  \n",
       "3                        putangina mo binay takbo pa  \n",
       "4  binay with selective amnesia  forgetting about...  \n",
       "5  it doesn t matter whoever won between duterte ...  \n",
       "6  nognog  pero nognog din ang nag malasakit  wtf...  \n",
       "7                                                     \n",
       "8    what abi binay said on running for makati mayor  \n",
       "9  srsly  how can binay do away with no tax for t...  "
      ]
     },
     "execution_count": 534,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# append cleaned tweets to the training data\n",
    "train[\"clean_tweet\"] = train_tweet\n",
    "\n",
    "# compare the cleaned and uncleaned tweets\n",
    "train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "244"
      ]
     },
     "execution_count": 535,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(train['clean_tweet'] == '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['Another'] = df['Another'].replace('', np.nan)\n",
    "#replace all empty spaces with NaN to drop using dropna\n",
    "train['clean_tweet'] = train['clean_tweet'].replace('', np.NaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>clean_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Inaasahan na ni Vice President Jejomar Binay n...</td>\n",
       "      <td>0</td>\n",
       "      <td>inaasahan na ni vice president jejomar binay n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mar Roxas TANG INA TUWID NA DAAN DAW .. EH SYA...</td>\n",
       "      <td>1</td>\n",
       "      <td>mar roxas tang ina tuwid na daan daw    eh sya...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Salamat sa walang sawang suporta ng mga taga m...</td>\n",
       "      <td>0</td>\n",
       "      <td>salamat sa walang sawang suporta ng mga taga m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@rapplerdotcom putangina mo binay TAKBO PA</td>\n",
       "      <td>1</td>\n",
       "      <td>putangina mo binay takbo pa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Binay with selective amnesia, forgetting about...</td>\n",
       "      <td>0</td>\n",
       "      <td>binay with selective amnesia  forgetting about...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4994</th>\n",
       "      <td>@jefk_rew pity those poor people who were invo...</td>\n",
       "      <td>1</td>\n",
       "      <td>pity those poor people who were involved in th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>Awww DUTERTE Na wag Lang si Roxas</td>\n",
       "      <td>0</td>\n",
       "      <td>awww duterte na wag lang si roxas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>RT @mikkieugenio: If the SC disqualifies Poe a...</td>\n",
       "      <td>0</td>\n",
       "      <td>if the sc disqualifies poe as president  eit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>\"Pag naging presidente si Binay, wala kayong t...</td>\n",
       "      <td>0</td>\n",
       "      <td>pag naging presidente si binay  wala kayong t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>Yan na naman ang walang kwentang commercial ni...</td>\n",
       "      <td>1</td>\n",
       "      <td>yan na naman ang walang kwentang commercial ni...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4756 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  label   \n",
       "0     Inaasahan na ni Vice President Jejomar Binay n...      0  \\\n",
       "1     Mar Roxas TANG INA TUWID NA DAAN DAW .. EH SYA...      1   \n",
       "2     Salamat sa walang sawang suporta ng mga taga m...      0   \n",
       "3            @rapplerdotcom putangina mo binay TAKBO PA      1   \n",
       "4     Binay with selective amnesia, forgetting about...      0   \n",
       "...                                                 ...    ...   \n",
       "4994  @jefk_rew pity those poor people who were invo...      1   \n",
       "4995                  Awww DUTERTE Na wag Lang si Roxas      0   \n",
       "4996  RT @mikkieugenio: If the SC disqualifies Poe a...      0   \n",
       "4997  \"Pag naging presidente si Binay, wala kayong t...      0   \n",
       "4998  Yan na naman ang walang kwentang commercial ni...      1   \n",
       "\n",
       "                                            clean_tweet  \n",
       "0     inaasahan na ni vice president jejomar binay n...  \n",
       "1     mar roxas tang ina tuwid na daan daw    eh sya...  \n",
       "2     salamat sa walang sawang suporta ng mga taga m...  \n",
       "3                           putangina mo binay takbo pa  \n",
       "4     binay with selective amnesia  forgetting about...  \n",
       "...                                                 ...  \n",
       "4994  pity those poor people who were involved in th...  \n",
       "4995                  awww duterte na wag lang si roxas  \n",
       "4996    if the sc disqualifies poe as president  eit...  \n",
       "4997   pag naging presidente si binay  wala kayong t...  \n",
       "4998  yan na naman ang walang kwentang commercial ni...  \n",
       "\n",
       "[4756 rows x 3 columns]"
      ]
     },
     "execution_count": 537,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.dropna(axis='rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://statisticsglobe.com/drop-rows-blank-values-from-pandas-dataframe-python\n",
    "train['clean_tweet'] = train['clean_tweet'].replace('', float('NaN'), regex = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.dropna(inplace= True)\n",
    "train = train.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_column = train.pop('label')\n",
    "train.insert(0,'label',first_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>clean_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Inaasahan na ni Vice President Jejomar Binay n...</td>\n",
       "      <td>inaasahan na ni vice president jejomar binay n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Mar Roxas TANG INA TUWID NA DAAN DAW .. EH SYA...</td>\n",
       "      <td>mar roxas tang ina tuwid na daan daw    eh sya...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Salamat sa walang sawang suporta ng mga taga m...</td>\n",
       "      <td>salamat sa walang sawang suporta ng mga taga m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>@rapplerdotcom putangina mo binay TAKBO PA</td>\n",
       "      <td>putangina mo binay takbo pa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Binay with selective amnesia, forgetting about...</td>\n",
       "      <td>binay with selective amnesia  forgetting about...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>It doesn't matter whoever won between Duterte ...</td>\n",
       "      <td>it doesn t matter whoever won between duterte ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>Nognog? Pero nognog din ang nag malasakit? Wtf...</td>\n",
       "      <td>nognog  pero nognog din ang nag malasakit  wtf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>#OnlyB1nay ?? #FB https://t.co/QEQnsK67Gm</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>What Abi Binay said on running for Makati mayo...</td>\n",
       "      <td>what abi binay said on running for makati mayor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>Srsly. How can Binay do away with no tax for t...</td>\n",
       "      <td>srsly  how can binay do away with no tax for t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text   \n",
       "0      0  Inaasahan na ni Vice President Jejomar Binay n...  \\\n",
       "1      1  Mar Roxas TANG INA TUWID NA DAAN DAW .. EH SYA...   \n",
       "2      0  Salamat sa walang sawang suporta ng mga taga m...   \n",
       "3      1         @rapplerdotcom putangina mo binay TAKBO PA   \n",
       "4      0  Binay with selective amnesia, forgetting about...   \n",
       "5      0  It doesn't matter whoever won between Duterte ...   \n",
       "6      1  Nognog? Pero nognog din ang nag malasakit? Wtf...   \n",
       "7      1          #OnlyB1nay ?? #FB https://t.co/QEQnsK67Gm   \n",
       "8      0  What Abi Binay said on running for Makati mayo...   \n",
       "9      1  Srsly. How can Binay do away with no tax for t...   \n",
       "\n",
       "                                         clean_tweet  \n",
       "0  inaasahan na ni vice president jejomar binay n...  \n",
       "1  mar roxas tang ina tuwid na daan daw    eh sya...  \n",
       "2  salamat sa walang sawang suporta ng mga taga m...  \n",
       "3                        putangina mo binay takbo pa  \n",
       "4  binay with selective amnesia  forgetting about...  \n",
       "5  it doesn t matter whoever won between duterte ...  \n",
       "6  nognog  pero nognog din ang nag malasakit  wtf...  \n",
       "7                                                     \n",
       "8    what abi binay said on running for makati mayor  \n",
       "9  srsly  how can binay do away with no tax for t...  "
      ]
     },
     "execution_count": 541,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4756, 3)\n"
     ]
    }
   ],
   "source": [
    "#total data entries for training\n",
    "\n",
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['akin', 'aking', 'ako', 'alin', 'am', 'amin', 'aming', 'ang', 'ano', 'anumang', 'apat', 'at', 'atin', 'ating', 'ay', 'bababa', 'bago', 'bakit', 'bawat', 'bilang', 'dahil', 'dalawa', 'dapat', 'din', 'dito', 'doon', 'gagawin', 'gayunman', 'ginagawa', 'ginawa', 'ginawang', 'gumawa', 'gusto', 'habang', 'hanggang', 'hindi', 'huwag', 'iba', 'ibaba', 'ibabaw', 'ibig', 'ikaw', 'ilagay', 'ilalim', 'ilan', 'inyong', 'isa', 'isang', 'itaas', 'ito', 'iyo', 'iyon', 'iyong', 'ka', 'kahit', 'kailangan', 'kailanman', 'kami', 'kanila', 'kanilang', 'kanino', 'kanya', 'kanyang', 'kapag', 'kapwa', 'karamihan', 'katiyakan', 'katulad', 'kaya', 'kaysa', 'ko', 'kong', 'kulang', 'kumuha', 'kung', 'laban', 'lahat', 'lamang', 'likod', 'lima', 'maaari', 'maaaring', 'maging', 'mahusay', 'makita', 'marami', 'marapat', 'masyado', 'may', 'mayroon', 'mga', 'minsan', 'mismo', 'mula', 'muli', 'na', 'nabanggit', 'naging', 'nagkaroon', 'nais', 'nakita', 'namin', 'napaka', 'narito', 'nasaan', 'ng', 'ngayon', 'ni', 'nila', 'nilang', 'nito', 'niya', 'niyang', 'noon', 'o', 'pa', 'paano', 'pababa', 'paggawa', 'pagitan', 'pagkakaroon', 'pagkatapos', 'palabas', 'pamamagitan', 'panahon', 'pangalawa', 'para', 'paraan', 'pareho', 'pataas', 'pero', 'pumunta', 'pumupunta', 'sa', 'saan', 'sabi', 'sabihin', 'sarili', 'sila', 'sino', 'siya', 'tatlo', 'tayo', 'tulad', 'tungkol', 'una', 'walang']\n"
     ]
    }
   ],
   "source": [
    "#remove stopwords\n",
    "import urllib.request, json \n",
    "with urllib.request.urlopen(\"https://raw.githubusercontent.com/stopwords-iso/stopwords-tl/master/stopwords-tl.json\") as url:\n",
    "    stopwords = json.loads(url.read().decode())\n",
    "    print(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data['content2'] =data['Content'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwords)]))\n",
    "train['rm_stpwrds'] = train['clean_tweet'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwords) ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_tweet</th>\n",
       "      <th>rm_stpwrds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>inaasahan na ni vice president jejomar binay n...</td>\n",
       "      <td>inaasahan vice president jejomar binay taong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mar roxas tang ina tuwid na daan daw    eh sya...</td>\n",
       "      <td>mar roxas tang ina tuwid daan daw eh sya nga d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>salamat sa walang sawang suporta ng mga taga m...</td>\n",
       "      <td>salamat sawang suporta taga makati pagbabalik ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>putangina mo binay takbo pa</td>\n",
       "      <td>putangina mo binay takbo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>binay with selective amnesia  forgetting about...</td>\n",
       "      <td>binay with selective amnesia forgetting about ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>it doesn t matter whoever won between duterte ...</td>\n",
       "      <td>it doesn t matter whoever won between duterte ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>nognog  pero nognog din ang nag malasakit  wtf...</td>\n",
       "      <td>nognog nognog nag malasakit wtf tangina mo bin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>what abi binay said on running for makati mayor</td>\n",
       "      <td>what abi binay said on running for makati mayor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>srsly  how can binay do away with no tax for t...</td>\n",
       "      <td>srsly how can binay do away with no tax for th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         clean_tweet   \n",
       "0  inaasahan na ni vice president jejomar binay n...  \\\n",
       "1  mar roxas tang ina tuwid na daan daw    eh sya...   \n",
       "2  salamat sa walang sawang suporta ng mga taga m...   \n",
       "3                        putangina mo binay takbo pa   \n",
       "4  binay with selective amnesia  forgetting about...   \n",
       "5  it doesn t matter whoever won between duterte ...   \n",
       "6  nognog  pero nognog din ang nag malasakit  wtf...   \n",
       "7                                                      \n",
       "8    what abi binay said on running for makati mayor   \n",
       "9  srsly  how can binay do away with no tax for t...   \n",
       "\n",
       "                                          rm_stpwrds  \n",
       "0       inaasahan vice president jejomar binay taong  \n",
       "1  mar roxas tang ina tuwid daan daw eh sya nga d...  \n",
       "2  salamat sawang suporta taga makati pagbabalik ...  \n",
       "3                           putangina mo binay takbo  \n",
       "4  binay with selective amnesia forgetting about ...  \n",
       "5  it doesn t matter whoever won between duterte ...  \n",
       "6  nognog nognog nag malasakit wtf tangina mo bin...  \n",
       "7                                                     \n",
       "8    what abi binay said on running for makati mayor  \n",
       "9  srsly how can binay do away with no tax for th...  "
      ]
     },
     "execution_count": 545,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[['clean_tweet', 'rm_stpwrds']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [inaasahan, vice, president, jejomar, binay, t...\n",
       "1    [mar, roxas, tang, ina, tuwid, daan, daw, eh, ...\n",
       "2    [salamat, sawang, suporta, taga, makati, pagba...\n",
       "3                        [putangina, mo, binay, takbo]\n",
       "4    [binay, with, selective, amnesia, forgetting, ...\n",
       "Name: tokenize, dtype: object"
      ]
     },
     "execution_count": 546,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tokenization \n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "train['tokenize'] = train['rm_stpwrds'].apply(nltk.tokenize.WhitespaceTokenizer().tokenize) \n",
    "train['tokenize'].head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rm_stpwrds</th>\n",
       "      <th>tokenize</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>inaasahan vice president jejomar binay taong</td>\n",
       "      <td>[inaasahan, vice, president, jejomar, binay, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mar roxas tang ina tuwid daan daw eh sya nga d...</td>\n",
       "      <td>[mar, roxas, tang, ina, tuwid, daan, daw, eh, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>salamat sawang suporta taga makati pagbabalik ...</td>\n",
       "      <td>[salamat, sawang, suporta, taga, makati, pagba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>putangina mo binay takbo</td>\n",
       "      <td>[putangina, mo, binay, takbo]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>binay with selective amnesia forgetting about ...</td>\n",
       "      <td>[binay, with, selective, amnesia, forgetting, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>it doesn t matter whoever won between duterte ...</td>\n",
       "      <td>[it, doesn, t, matter, whoever, won, between, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>nognog nognog nag malasakit wtf tangina mo bin...</td>\n",
       "      <td>[nognog, nognog, nag, malasakit, wtf, tangina,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>what abi binay said on running for makati mayor</td>\n",
       "      <td>[what, abi, binay, said, on, running, for, mak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>srsly how can binay do away with no tax for th...</td>\n",
       "      <td>[srsly, how, can, binay, do, away, with, no, t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          rm_stpwrds   \n",
       "0       inaasahan vice president jejomar binay taong  \\\n",
       "1  mar roxas tang ina tuwid daan daw eh sya nga d...   \n",
       "2  salamat sawang suporta taga makati pagbabalik ...   \n",
       "3                           putangina mo binay takbo   \n",
       "4  binay with selective amnesia forgetting about ...   \n",
       "5  it doesn t matter whoever won between duterte ...   \n",
       "6  nognog nognog nag malasakit wtf tangina mo bin...   \n",
       "7                                                      \n",
       "8    what abi binay said on running for makati mayor   \n",
       "9  srsly how can binay do away with no tax for th...   \n",
       "\n",
       "                                            tokenize  \n",
       "0  [inaasahan, vice, president, jejomar, binay, t...  \n",
       "1  [mar, roxas, tang, ina, tuwid, daan, daw, eh, ...  \n",
       "2  [salamat, sawang, suporta, taga, makati, pagba...  \n",
       "3                      [putangina, mo, binay, takbo]  \n",
       "4  [binay, with, selective, amnesia, forgetting, ...  \n",
       "5  [it, doesn, t, matter, whoever, won, between, ...  \n",
       "6  [nognog, nognog, nag, malasakit, wtf, tangina,...  \n",
       "7                                                 []  \n",
       "8  [what, abi, binay, said, on, running, for, mak...  \n",
       "9  [srsly, how, can, binay, do, away, with, no, t...  "
      ]
     },
     "execution_count": 547,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[['rm_stpwrds','tokenize']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokenize</th>\n",
       "      <th>lematize_nltk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2892</th>\n",
       "      <td>[11, 22, 53, pht]</td>\n",
       "      <td>[11, 22, 53, pht]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1008</th>\n",
       "      <td>[basta, taga, pasig, mandaluyong, pogi, magand...</td>\n",
       "      <td>[basta, taga, pasig, mandaluyong, pogi, magand...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>952</th>\n",
       "      <td>[daming, polls, ayaw, isama, si, binay, di, ba...</td>\n",
       "      <td>[daming, poll, ayaw, isama, si, binay, di, ba,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3261</th>\n",
       "      <td>[si, mar, envelope, si, binay, lantarang, vote...</td>\n",
       "      <td>[si, mar, envelope, si, binay, lantarang, vote...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3951</th>\n",
       "      <td>[mas, maganda, pang, iboto, si, alma, moreno, ...</td>\n",
       "      <td>[ma, maganda, pang, iboto, si, alma, moreno, k...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tokenize   \n",
       "2892                                  [11, 22, 53, pht]  \\\n",
       "1008  [basta, taga, pasig, mandaluyong, pogi, magand...   \n",
       "952   [daming, polls, ayaw, isama, si, binay, di, ba...   \n",
       "3261  [si, mar, envelope, si, binay, lantarang, vote...   \n",
       "3951  [mas, maganda, pang, iboto, si, alma, moreno, ...   \n",
       "\n",
       "                                          lematize_nltk  \n",
       "2892                                  [11, 22, 53, pht]  \n",
       "1008  [basta, taga, pasig, mandaluyong, pogi, magand...  \n",
       "952   [daming, poll, ayaw, isama, si, binay, di, ba,...  \n",
       "3261  [si, mar, envelope, si, binay, lantarang, vote...  \n",
       "3951  [ma, maganda, pang, iboto, si, alma, moreno, k...  "
      ]
     },
     "execution_count": 548,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "def lema_words(text):\n",
    "  wnl=WordNetLemmatizer()\n",
    "  return[wnl.lemmatize(w) for w in text]\n",
    "\n",
    "train['lematize_nltk']=train['tokenize'].apply(lema_words)  \n",
    "train[['tokenize','lematize_nltk']].sample(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proceeding to Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOWELS = \"aeiouAEIOU\"\n",
    "CONSONANTS = \"bcdfghklmnngpqrstvwyBCDFGHKLMNNGPQRSTVWY\"\n",
    "\n",
    "\"\"\" \n",
    "\tAffixes\n",
    "\"\"\"\n",
    "PREFIX_SET = [\n",
    "\t'nakikipag', 'pakikipag',\n",
    "\t'pinakama', 'pagpapa',\n",
    "\t'pinagka', 'panganga', \n",
    "\t'makapag', 'nakapag', \n",
    "\t'tagapag', 'makipag', \n",
    "\t'nakipag', 'tigapag',\n",
    "\t'pakiki', 'magpa',\n",
    "\t'napaka', 'pinaka',\n",
    "\t'ipinag', 'pagka', \n",
    "\t'pinag', 'mapag', \n",
    "\t'mapa', 'taga', \n",
    "\t'ipag', 'tiga', \n",
    "\t'pala', 'pina', \n",
    "\t'pang', 'naka',\n",
    "\t'nang', 'mang',\n",
    "\t'sing',\n",
    "\t'ipa', 'pam',\n",
    "\t'pan', 'pag',\n",
    "\t'tag', 'mai',\n",
    "\t'mag', 'nam',\n",
    "\t'nag', 'man',\n",
    "\t'may', 'ma',\n",
    "\t'na', 'ni',\n",
    "\t'pa', 'ka',\n",
    "\t'um', 'in',\n",
    "\t'i',\n",
    "]\n",
    "\n",
    "INFIX_SET = [\n",
    "\t'um', 'in',\n",
    "]\n",
    "\n",
    "SUFFIX_SET = [\n",
    "\t'syon','dor', \n",
    "\t'ita', 'han', \n",
    "\t'hin', 'ing', \n",
    "\t'ang', 'ng', \n",
    "\t'an', 'in', \n",
    "\t'g',\n",
    "]\n",
    "\n",
    "PERIOD_FLAG = True\n",
    "PASS_FLAG = False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def check_vowel(substring):\n",
    "\t\"\"\"\n",
    "\t\tChecks if the substring is a vowel.\n",
    "\t\t\tletters: substring to be tested\n",
    "\t\treturns BOOLEAN\n",
    "\t\"\"\"\n",
    "\n",
    "\treturn all(letter in VOWELS for letter in substring)\n",
    "\n",
    "\n",
    "def check_consonant(substring):\n",
    "\t\"\"\"\n",
    "\t\tChecks if the letter is a consonant.\n",
    "\t\t\tletter: substring to be tested\n",
    "\t\treturns BOOLEAN\n",
    "\t\"\"\"\n",
    "\n",
    "\treturn all(letter in CONSONANTS for letter in substring)\n",
    "\n",
    "def change_letter(token, index, letter):\n",
    "\t\"\"\"\n",
    "\t\tReplaces a letter in a token.\n",
    "\t\t\ttoken: word to be used\n",
    "\t\t\tindex: index of the letter\n",
    "\t\t\tletter: letter used to replace\n",
    "\t\treturns STRING\n",
    "\t\"\"\"\n",
    "\t\n",
    "\t_list = list(token)\n",
    "\t_list[index] = letter\n",
    "\n",
    "\treturn ''.join(_list)\n",
    "\n",
    "def count_vowel(token):\n",
    "\t\"\"\"\n",
    "\t\tCount vowels in a given token.\n",
    "\t\t\ttoken: string to be counted for vowels\n",
    "\t\treturns INTEGER\n",
    "\t\"\"\"\n",
    "\n",
    "\tcount = 0\n",
    "\n",
    "\tfor tok in token:\n",
    "\t\tif check_vowel(tok):\n",
    "\t\t\tcount+=1\n",
    "\n",
    "\treturn count\n",
    "\n",
    "\n",
    "def count_consonant(token):\n",
    "\t\"\"\"\n",
    "\t\tCount consonants in a given token.\n",
    "\t\t\ttoken: string to be counted for consonants\n",
    "\t\treturns INTEGER\n",
    "\t\"\"\"\n",
    "\n",
    "\tcount = 0\n",
    "\n",
    "\tfor tok in token:\n",
    "\t\tif check_consonant(tok):\n",
    "\t\t\tcount+=1\n",
    "\n",
    "\treturn count\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def check_validation(token):\n",
    "    with open('stemmer/validation.txt', 'r') as valid:\n",
    "        data = valid.read().replace('\\n', ' ').split(' ')\n",
    "\n",
    "    return token in data\n",
    "\n",
    "\n",
    "\n",
    "def clean_repitition(token, REPITITION):\n",
    "\t\"\"\"\n",
    "\t\tChecks token for repitition. (ex. nakakabaliw = nabaliw)\n",
    "\t\t\ttoken: word to be stemmed repitition\n",
    "\t\treturns STRING\n",
    "\t\"\"\"\n",
    "\n",
    "\tif check_validation(token):\n",
    "\t\treturn token\n",
    "\n",
    "\tif len(token) >= 4:\n",
    "\t\tif check_vowel(token[0]):\n",
    "\t\t\tif token[0] == token[1]:\n",
    "\t\t\t\tREPITITION.append(token[0])\n",
    "\t\t\t\treturn token[1:]\n",
    "\n",
    "\t\telif check_consonant(token[0]) and count_vowel(token) >= 2:\n",
    "\t\t\tif token[0: 2] == token[2: 4] and len(token) - 2 >= 4:\n",
    "\t\t\t\tREPITITION.append(token[2:4])\n",
    "\t\t\t\treturn token[2:]\n",
    "\t\t\t\n",
    "\t\t\telif token[0: 3] == token[3: 6] and len(token) - 3 >= 4:\n",
    "\t\t\t\tREPITITION.append(token[3:6])\n",
    "\t\t\t\treturn token[3:]\n",
    "\n",
    "\treturn token\n",
    "\n",
    "def clean_suffix(token, SUFFIX):\n",
    "    \"\"\"\n",
    "    Checks token for suffixes. (ex. bigayan = bigay)\n",
    "        token: word to be stemmed for suffixes\n",
    "    returns STRING\n",
    "    \"\"\"\n",
    "\n",
    "    SUF_CANDIDATE = []\n",
    "\n",
    "    if check_validation(token):\n",
    "        return token\n",
    "\n",
    "    for suffix in SUFFIX_SET:\n",
    "        if len(token) - len(suffix) >= 3 and count_vowel(token[0:len(token) - len(suffix)]) >= 2 and count_consonant(token[0:len(token) - len(suffix)]) >= 1:\n",
    "            if token[len(token) - len(suffix): len(token)] == suffix:\n",
    "                if len(suffix) == 2 and not count_consonant(token[0:len(token) - len(suffix)]) >= 1:\n",
    "                    continue\n",
    "\n",
    "                if count_vowel(token[0: len(token) - len(suffix)]) >= 2:\n",
    "                    if suffix == 'ang' and check_consonant(token[-4]) \\\n",
    "                            and token[-4] != 'r' and token[-5] != 'u':\n",
    "                        continue\n",
    "\n",
    "                    #print(token[0: len(token) - len(suffix)] + \" : \" + suffix)\n",
    "\n",
    "                    if check_validation(token[0: len(token) - len(suffix)]):\n",
    "                        SUFFIX.append(suffix)\n",
    "                        return token[0: len(token) - len(suffix)] + 'a' if suffix == 'ita' \\\n",
    "                            else token[0: len(token) - len(suffix)]\n",
    "\n",
    "                    elif len(SUF_CANDIDATE) == 0:\n",
    "                        SUF_CANDIDATE.append(suffix)\n",
    "                        SUF_CANDIDATE.append(token[0: len(token) - len(suffix)])\n",
    "\n",
    "    if (len(SUF_CANDIDATE) == 2):\n",
    "        SUFFIX = SUF_CANDIDATE[0]\n",
    "        return SUF_CANDIDATE[1][0: len(token) - len(SUFFIX)] + 'a' if SUFFIX == 'ita' \\\n",
    "            else SUF_CANDIDATE[1][0: len(token) - len(SUFFIX)]\n",
    "\n",
    "    return token\n",
    "\n",
    "\n",
    "def clean_infix(token, INFIX):\n",
    "\t\"\"\"\n",
    "\t\tChecks token for infixes. (ex. bumalik = balik)\n",
    "\t\t\ttoken: word to be stemmed for infixes\n",
    "\t\treturns STRING\n",
    "\t\"\"\"\n",
    "\n",
    "\tif check_validation(token):\n",
    "\t\treturn token\n",
    "\n",
    "\tfor infix in INFIX_SET:\n",
    "\t\tif len(token) - len(infix) >= 3 and count_vowel(token[len(infix):]) >= 2:\n",
    "\t\t\tif token[0] == token[4] and token[1: 4] == infix:\n",
    "\t\t\t\tINFIX.append(infix)\n",
    "\t\t\t\treturn token[4:]\n",
    "\n",
    "\t\t\telif token[2] == token[4] and token[1: 3] == infix:\n",
    "\t\t\t\tINFIX.append(infix)\n",
    "\t\t\t\treturn token[0] + token[3:]\n",
    "\n",
    "\t\t\telif token[1: 3] == infix and check_vowel(token[3]):\n",
    "\t\t\t\tINFIX.append(infix)\n",
    "\t\t\t\treturn token[0] + token[3:]\n",
    "\n",
    "\treturn token\n",
    "\n",
    "\n",
    "def clean_prefix(token,\t PREFIX):\n",
    "\t\"\"\"\n",
    "\t\tChecks token for prefixes. (ex. naligo = ligo)\n",
    "\t\t\ttoken: word to be stemmed for prefixes\n",
    "\t\treturns STRING\n",
    "\t\"\"\"\n",
    "\n",
    "\tif check_validation(token):\n",
    "\t\treturn token\n",
    "\n",
    "\tfor prefix in PREFIX_SET:\n",
    "\t\tif len(token) - len(prefix) >= 3 and \\\n",
    "\t\t\tcount_vowel(token[len(prefix):]) >= 2:\n",
    "\n",
    "\t\t\tif prefix == ('i') and check_consonant(token[2]):\n",
    "\t\t\t\tcontinue\n",
    "\n",
    "\t\t\tif '-' in token:\t\n",
    "\t\t\t\ttoken = token.split('-')\n",
    "\n",
    "\t\t\t\tif token[0] == prefix and check_vowel(token[1][0]):\n",
    "\t\t\t\t\tPREFIX.append(prefix)\n",
    "\t\t\t\t\treturn token[1]\n",
    "\n",
    "\t\t\t\ttoken = '-'.join(token)\n",
    "\n",
    "\t\t\tif token[0: len(prefix)] == prefix:\n",
    "\t\t\t\tif count_vowel(token[len(prefix):]) >= 2:\n",
    "\t\t\t\t\t# if check_vowel(token[len(token) - len(prefix) - 1]):\n",
    "\t\t\t\t# \tcontinue\n",
    "\n",
    "\t\t\t\t\tif prefix == 'panganga':\n",
    "\t\t\t\t\t\tPREFIX.append(prefix)\n",
    "\t\t\t\t\t\treturn 'ka' + token[len(prefix):]\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\tPREFIX.append(prefix)\n",
    "\t\t\t\t\treturn token[len(prefix):]\n",
    "\n",
    "\treturn token\n",
    "\n",
    "\n",
    "def clean_duplication(token, DUPLICATE):\n",
    "\t\"\"\"\n",
    "\t\tChecks token for duplication. (ex. araw-araw = araw)\n",
    "\t\t\ttoken: word to be stemmed duplication\n",
    "\t\treturns STRING\n",
    "\t\"\"\"\n",
    "\n",
    "\tif check_validation(token):\n",
    "\t\treturn token\n",
    "\n",
    "\tif '-' in token and token.index('-') != 0 and \\\n",
    "\t\ttoken.index('-') != len(token) -  1:\n",
    "\n",
    "\t\tsplit = token.split('-')\n",
    "\n",
    "\t\tif all(len(tok) >= 3 for tok in split):\n",
    "\t\t\tif split[0] == token[1] or split[0][-1] == 'u' and change_letter(split[0], -1, 'o') == split[1] or \\\n",
    "\t\t\t\tsplit[0][-2] == 'u' and change_letter(split[0], -2, 'o')  == split[1]:\n",
    "\t\t\t\tDUPLICATE.append(split[0])\n",
    "\t\t\t\treturn split[0]\n",
    "\n",
    "\t\t\telif split[0] == split[1][0:len(split[0])]:\n",
    "\t\t\t\tDUPLICATE.append(split[1])\n",
    "\t\t\t\treturn split[1]\n",
    "\n",
    "\t\t\telif split[0][-2:] == 'ng':\n",
    "\t\t\t\tif split[0][-3] == 'u':\n",
    "\t\t\t\t\tif split[0][0:-3] + 'o' == split[1]:\n",
    "\t\t\t\t\t\tDUPLICATE.append(split[1])\n",
    "\t\t\t\t\t\treturn split[1]\n",
    "\n",
    "\t\t\t\tif split[0][0:-2] == split[1]:\n",
    "\t\t\t\t\tDUPLICATE.append(split[1])\n",
    "\t\t\t\t\treturn split[1]\n",
    "\n",
    "\t\telse:\n",
    "\t\t\treturn '-'.join(split)\n",
    "\t\n",
    "\treturn token\n",
    "\n",
    "\n",
    "def clean_repitition(token, REPITITION):\n",
    "\t\"\"\"\n",
    "\t\tChecks token for repitition. (ex. nakakabaliw = nabaliw)\n",
    "\t\t\ttoken: word to be stemmed repitition\n",
    "\t\treturns STRING\n",
    "\t\"\"\"\n",
    "\n",
    "\tif check_validation(token):\n",
    "\t\treturn token\n",
    "\n",
    "\tif len(token) >= 4:\n",
    "\t\tif check_vowel(token[0]):\n",
    "\t\t\tif token[0] == token[1]:\n",
    "\t\t\t\tREPITITION.append(token[0])\n",
    "\t\t\t\treturn token[1:]\n",
    "\n",
    "\t\telif check_consonant(token[0]) and count_vowel(token) >= 2:\n",
    "\t\t\tif token[0: 2] == token[2: 4] and len(token) - 2 >= 4:\n",
    "\t\t\t\tREPITITION.append(token[2:4])\n",
    "\t\t\t\treturn token[2:]\n",
    "\t\t\t\n",
    "\t\t\telif token[0: 3] == token[3: 6] and len(token) - 3 >= 4:\n",
    "\t\t\t\tREPITITION.append(token[3:6])\n",
    "\t\t\t\treturn token[3:]\n",
    "\n",
    "\treturn token\n",
    "\n",
    "\n",
    "def clean_stemmed(token, CLEANERS, REPITITION):\n",
    "\t\t\n",
    "\tif not token:\n",
    "\t\treturn \"\"\n",
    "\t\n",
    "\t\"\"\"\n",
    "\t\tChecks for left-over affixes and letters.\n",
    "\t\t\ttoken: word to be cleaned for excess affixes/letters\n",
    "\t\treturns STRING\n",
    "\t\"\"\"\n",
    "\n",
    "\tglobal PERIOD_FLAG\n",
    "\tglobal PASS_FLAG\n",
    "\n",
    "\tCC_EXP = ['dr', 'gl', 'gr', 'ng', 'kr', 'kl', 'kw', 'ts', 'tr', 'pr', 'pl', 'pw', 'sw', 'sy'] # Consonant + Consonant Exceptions\n",
    "\n",
    "\tif token[-1] == '.' and PASS_FLAG == False:\n",
    "\t\tPERIOD_FLAG = True\n",
    "\n",
    "\tif not check_vowel(token[-1]) and not check_consonant(token[-1]):\n",
    "\t\tCLEANERS.append(token[-1])\n",
    "\t\ttoken = token[0:-1]\n",
    "\n",
    "#\tif not check_vowel(token[0]) and not check_consonant(token[0]):\n",
    "#\t\tCLEANERS.append(token[0])\n",
    "#\t\ttoken = token[1:]\n",
    "\n",
    "\tif check_validation(token):\n",
    "\t\treturn token\n",
    "\n",
    "\tif len(token) >= 3 and count_vowel(token) >= 2:\n",
    "\t\ttoken = clean_repitition(token,\tREPITITION)\n",
    "\n",
    "\t\tif check_consonant(token[-1]) and token[- 2] == 'u':\n",
    "\t\t\tCLEANERS.append('u')\n",
    "\t\t\ttoken = change_letter(token, -2, 'o')\n",
    "\n",
    "\t\tif token[len(token) - 1] == 'u':\n",
    "\t\t\tCLEANERS.append('u')\n",
    "\t\t\ttoken = change_letter(token, -1, 'o')\n",
    "\n",
    "\t\tif token[-1] == 'r':\n",
    "\t\t\tCLEANERS.append('r')\n",
    "\t\t\ttoken = change_letter(token, -1, 'd')\n",
    "\n",
    "\t\tif token[-1] == 'h' and check_vowel(token[-1]):\n",
    "\t\t\tCLEANERS.append('h')\n",
    "\t\t\ttoken = token[0:-1]\n",
    "\n",
    "\t\t# if token[0] == 'i':\n",
    "\t\t# \ttoken = token[1:]\n",
    "\n",
    "\t\tif token[0] == token[1]:\n",
    "\t\t\tCLEANERS.append(token[0])\n",
    "\t\t\ttoken = token[1:]\n",
    "\n",
    "\t\tif (token[0: 2] == 'ka' or token[0: 2] == 'pa') and check_consonant(token[2]) \\\n",
    "\t\t\tand count_vowel(token) >= 3:\n",
    "\t\t\t\n",
    "\t\t\tCLEANERS.append(token[0: 2])\n",
    "\t\t\ttoken = token[2:]\n",
    "\n",
    "\t\tif(token[-3:]) == 'han' and count_vowel(token[0:-3]) == 1:\n",
    "\t\t\tCLEANERS.append('han')\n",
    "\t\t\ttoken = token[0:-3] + 'i'\n",
    "\n",
    "\t\tif(token[-3:]) == 'han' and count_vowel(token[0:-3]) > 1:\n",
    "\t\t\tCLEANERS.append('han')\n",
    "\t\t\ttoken = token[0:-3]\n",
    "\n",
    "\t\tif len(token) >= 2 and count_vowel(token) >= 3:\n",
    "\t\t\tif token[-1] == 'h' and check_vowel(token[-2]):\n",
    "\t\t\t\tCLEANERS.append('h')\n",
    "\t\t\t\ttoken = token[0:-1]\n",
    "\n",
    "\t\tif len(token) >= 6 and token[0:2] == token[2:4]:\n",
    "\t\t\tCLEANERS.append('0:2')\n",
    "\t\t\ttoken = token[2:]\n",
    "\n",
    "\t\tif any(REP[0] == 'r' for REP in REPITITION):\n",
    "\t\t\tCLEANERS.append('r')\n",
    "\t\t\ttoken = change_letter(token, 0, 'd')\n",
    "\n",
    "\t\tif token[-2:] == 'ng' and token[-3] == 'u':\n",
    "\t\t\tCLEANERS.append('u')\n",
    "\t\t\ttoken = change_letter(token, -3, 'o')\n",
    "\n",
    "\t\tif token[-1] == 'h':\n",
    "\t\t\tCLEANERS.append('h')\n",
    "\t\t\ttoken = token[0:-1]\n",
    "\n",
    "\t\tif any(token[0:2] != CC for CC in CC_EXP) and check_consonant(token[0:2]):\n",
    "\t\t\tCLEANERS.append(token[0:2])\n",
    "\t\t\ttoken = token[1:]\n",
    "\n",
    "\treturn token\n",
    "\n",
    "\n",
    "def tg_stemmer(tokens):\n",
    "\n",
    "    global PERIOD_FLAG\n",
    "    global PASS_FLAG\n",
    "\n",
    "    pre_stem     = inf_stem = suf_stem = rep_stem = \\\n",
    "        du1_stem = du2_stem = cle_stem = '-'\n",
    "    word_info    = {}\n",
    "    PREFIX     = []\n",
    "    INFIX      = []\n",
    "    SUFFIX     = []\n",
    "    DUPLICATE  = []\n",
    "    REPITITION = []\n",
    "    CLEANERS   = []\n",
    "\n",
    "    word_info['clean'] = '-'\n",
    "    stemmed_tokens = []\n",
    "\n",
    "    for token in tokens:\n",
    "        word_info = {}\n",
    "        word_info[\"word\"] = token\n",
    "\n",
    "        if (PERIOD_FLAG == True and token[0].isupper()) or \\\n",
    "                (PERIOD_FLAG == False and token[0].islower()):\n",
    "            token = token.lower()\n",
    "            du1_stem = clean_duplication(token, DUPLICATE)\n",
    "            pre_stem = clean_prefix(du1_stem, PREFIX)\n",
    "            rep_stem = clean_repitition(pre_stem, REPITITION)\n",
    "            inf_stem = clean_infix(rep_stem, INFIX)\n",
    "            rep_stem = clean_repitition(inf_stem, REPITITION)\n",
    "            suf_stem = clean_suffix(rep_stem, SUFFIX)\n",
    "            du2_stem = clean_duplication(suf_stem, DUPLICATE)\n",
    "            cle_stem = clean_stemmed(du2_stem, CLEANERS, REPITITION)\n",
    "            cle_stem = clean_duplication(cle_stem, DUPLICATE)\n",
    "\n",
    "            if '-' in cle_stem:\n",
    "                cle_stem.replace('-', '')\n",
    "\n",
    "        else:\n",
    "            PERIOD_FLAG = False\n",
    "            cle_stem = clean_stemmed(token, CLEANERS, REPITITION)\n",
    "            word_info[\"root\"]   = token\n",
    "            word_info[\"prefix\"] = '[]'\n",
    "            word_info[\"infix\"]  = '[]'\n",
    "            word_info[\"suffix\"] = '[]'\n",
    "            word_info[\"repeat\"] = '[]'\n",
    "            word_info[\"dupli\"]  = '[]'\n",
    "            word_info[\"clean\"]  = cle_stem\n",
    "\n",
    "        stemmed_tokens.append(cle_stem)\n",
    "\n",
    "    return stemmed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ENGLISH STEMMER\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def english_lemmatizer(token):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return lemmatizer.lemmatize(token)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 551,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_tagalog_english(tokens):\n",
    "    lemmatized_tokens = []\n",
    "\n",
    "    for token in tokens:\n",
    "        if wordnet.synsets(token):\n",
    "            pos = nltk.pos_tag([token])[0][1][0].lower()\n",
    "            pos = {'a': wordnet.ADJ,\n",
    "                   'n': wordnet.NOUN,\n",
    "                   'v': wordnet.VERB,\n",
    "                   'r': wordnet.ADV}.get(pos, wordnet.NOUN)\n",
    "            lemmatized_token = lemmatizer.lemmatize(token, pos)\n",
    "        else:\n",
    "            lemmatized_token = tg_stemmer([token])[0]\n",
    "        \n",
    "        lemmatized_tokens.append(lemmatized_token)\n",
    "\n",
    "    return lemmatized_tokens\n",
    "\n",
    "train['lemmatize'] = train['tokenize'].apply(lemmatize_tagalog_english)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>rm_stpwrds</th>\n",
       "      <th>tokenize</th>\n",
       "      <th>lemmatize</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4746</th>\n",
       "      <td>1</td>\n",
       "      <td>Mar Roxas on MRT \"Yan na ang nadatnan natin.\" ...</td>\n",
       "      <td>mar roxas on mrt yan nadatnan natin puta di ga...</td>\n",
       "      <td>[mar, roxas, on, mrt, yan, nadatnan, natin, pu...</td>\n",
       "      <td>[mar, roxas, on, mrt, yan, datnan, natin, puta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4747</th>\n",
       "      <td>0</td>\n",
       "      <td>Bank robber turned multi-millionaire arrested ...</td>\n",
       "      <td>bank robber turned multi millionaire arrested ...</td>\n",
       "      <td>[bank, robber, turned, multi, millionaire, arr...</td>\n",
       "      <td>[bank, robber, turn, multi, millionaire, arres...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4748</th>\n",
       "      <td>1</td>\n",
       "      <td>Nakakairita yung tvc na para kay binay. Hehe</td>\n",
       "      <td>nakakairita yung tvc kay binay hehe</td>\n",
       "      <td>[nakakairita, yung, tvc, kay, binay, hehe]</td>\n",
       "      <td>[kaira, yung, tvc, kay, binay, hehe]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4749</th>\n",
       "      <td>1</td>\n",
       "      <td>SOBRA ?????? Puro sila Roxas samantalang pag k...</td>\n",
       "      <td>sobra puro roxas samantalang pag kay duterte p...</td>\n",
       "      <td>[sobra, puro, roxas, samantalang, pag, kay, du...</td>\n",
       "      <td>[sobra, puro, roxas, samantala, pag, kay, dute...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4750</th>\n",
       "      <td>0</td>\n",
       "      <td>ElectionHugot:“We lose the presidency not once...</td>\n",
       "      <td>electionhugot we lose the presidency not once ...</td>\n",
       "      <td>[electionhugot, we, lose, the, presidency, not...</td>\n",
       "      <td>[electionhugot, we, lose, the, presidency, not...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4751</th>\n",
       "      <td>1</td>\n",
       "      <td>@jefk_rew pity those poor people who were invo...</td>\n",
       "      <td>pity those poor people who were involved in th...</td>\n",
       "      <td>[pity, those, poor, people, who, were, involve...</td>\n",
       "      <td>[pity, hose, poor, people, who, be, involve, i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4752</th>\n",
       "      <td>0</td>\n",
       "      <td>Awww DUTERTE Na wag Lang si Roxas</td>\n",
       "      <td>awww duterte wag lang si roxas</td>\n",
       "      <td>[awww, duterte, wag, lang, si, roxas]</td>\n",
       "      <td>[awww, duterte, wag, lang, si, roxas]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4753</th>\n",
       "      <td>0</td>\n",
       "      <td>RT @mikkieugenio: If the SC disqualifies Poe a...</td>\n",
       "      <td>if the sc disqualifies poe as president either...</td>\n",
       "      <td>[if, the, sc, disqualifies, poe, as, president...</td>\n",
       "      <td>[if, the, sc, disqualifies, poe, a, president,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4754</th>\n",
       "      <td>0</td>\n",
       "      <td>\"Pag naging presidente si Binay, wala kayong t...</td>\n",
       "      <td>pag presidente si binay wala kayong tax lol it...</td>\n",
       "      <td>[pag, presidente, si, binay, wala, kayong, tax...</td>\n",
       "      <td>[pag, presidente, si, binay, wala, kayo, tax, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4755</th>\n",
       "      <td>1</td>\n",
       "      <td>Yan na naman ang walang kwentang commercial ni...</td>\n",
       "      <td>yan naman kwentang commercial vp binay</td>\n",
       "      <td>[yan, naman, kwentang, commercial, vp, binay]</td>\n",
       "      <td>[yan, naman, kwenta, commercial, vp, binay]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                               text   \n",
       "4746      1  Mar Roxas on MRT \"Yan na ang nadatnan natin.\" ...  \\\n",
       "4747      0  Bank robber turned multi-millionaire arrested ...   \n",
       "4748      1       Nakakairita yung tvc na para kay binay. Hehe   \n",
       "4749      1  SOBRA ?????? Puro sila Roxas samantalang pag k...   \n",
       "4750      0  ElectionHugot:“We lose the presidency not once...   \n",
       "4751      1  @jefk_rew pity those poor people who were invo...   \n",
       "4752      0                  Awww DUTERTE Na wag Lang si Roxas   \n",
       "4753      0  RT @mikkieugenio: If the SC disqualifies Poe a...   \n",
       "4754      0  \"Pag naging presidente si Binay, wala kayong t...   \n",
       "4755      1  Yan na naman ang walang kwentang commercial ni...   \n",
       "\n",
       "                                             rm_stpwrds   \n",
       "4746  mar roxas on mrt yan nadatnan natin puta di ga...  \\\n",
       "4747  bank robber turned multi millionaire arrested ...   \n",
       "4748                nakakairita yung tvc kay binay hehe   \n",
       "4749  sobra puro roxas samantalang pag kay duterte p...   \n",
       "4750  electionhugot we lose the presidency not once ...   \n",
       "4751  pity those poor people who were involved in th...   \n",
       "4752                     awww duterte wag lang si roxas   \n",
       "4753  if the sc disqualifies poe as president either...   \n",
       "4754  pag presidente si binay wala kayong tax lol it...   \n",
       "4755             yan naman kwentang commercial vp binay   \n",
       "\n",
       "                                               tokenize   \n",
       "4746  [mar, roxas, on, mrt, yan, nadatnan, natin, pu...  \\\n",
       "4747  [bank, robber, turned, multi, millionaire, arr...   \n",
       "4748         [nakakairita, yung, tvc, kay, binay, hehe]   \n",
       "4749  [sobra, puro, roxas, samantalang, pag, kay, du...   \n",
       "4750  [electionhugot, we, lose, the, presidency, not...   \n",
       "4751  [pity, those, poor, people, who, were, involve...   \n",
       "4752              [awww, duterte, wag, lang, si, roxas]   \n",
       "4753  [if, the, sc, disqualifies, poe, as, president...   \n",
       "4754  [pag, presidente, si, binay, wala, kayong, tax...   \n",
       "4755      [yan, naman, kwentang, commercial, vp, binay]   \n",
       "\n",
       "                                              lemmatize  \n",
       "4746  [mar, roxas, on, mrt, yan, datnan, natin, puta...  \n",
       "4747  [bank, robber, turn, multi, millionaire, arres...  \n",
       "4748               [kaira, yung, tvc, kay, binay, hehe]  \n",
       "4749  [sobra, puro, roxas, samantala, pag, kay, dute...  \n",
       "4750  [electionhugot, we, lose, the, presidency, not...  \n",
       "4751  [pity, hose, poor, people, who, be, involve, i...  \n",
       "4752              [awww, duterte, wag, lang, si, roxas]  \n",
       "4753  [if, the, sc, disqualifies, poe, a, president,...  \n",
       "4754  [pag, presidente, si, binay, wala, kayo, tax, ...  \n",
       "4755        [yan, naman, kwenta, commercial, vp, binay]  "
      ]
     },
     "execution_count": 553,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[['label','text','rm_stpwrds','tokenize','lemmatize']].tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(train['lemmatize'], train['label'], test_size=0.3, random_state=1)\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(map(' '.join, X_train))\n",
    "X_test = vectorizer.transform(map(' '.join, X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7266993693062369\n"
     ]
    }
   ],
   "source": [
    "# Train the SVM model\n",
    "clf = svm.SVC()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Dump the trained SVM model to a file using joblib\n",
    "joblib.dump(clf, 'svm_model.joblib')\n",
    "\n",
    "joblib.dump(vectorizer, 'tfidf_vectorizer.joblib')\n",
    "\n",
    "# Test the model on the testing set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "accuracy = clf.score(X_test, y_test)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tfidf_vectorizer.joblib']"
      ]
     },
     "execution_count": 559,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(clf, 'svm_model.joblib')\n",
    "\n",
    "joblib.dump(vectorizer, 'tfidf_vectorizer.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tangina pre makakagruate na kami salamat lord\n",
      "Positive sentiment\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "import urllib.request, json\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "# Download Filipino stopwords\n",
    "with urllib.request.urlopen(\"https://raw.githubusercontent.com/stopwords-iso/stopwords-tl/master/stopwords-tl.json\") as url:\n",
    "    stopwords_tl = json.loads(url.read().decode())\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Remove unwanted characters and digits\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    #Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Convert tokens to lowercase\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    # # Remove stopwords\n",
    "    tokens = [word for word in tokens if word.lower() not in stopwords_tl]\n",
    "    # Lemmatize the tokens\n",
    "    lemmatized_tokens = []\n",
    "\n",
    "    for token in tokens:\n",
    "        if wordnet.synsets(token):\n",
    "            pos = nltk.pos_tag([token])[0][1][0].lower()\n",
    "            pos = {'a': wordnet.ADJ,\n",
    "                   'n': wordnet.NOUN,\n",
    "                   'v': wordnet.VERB,\n",
    "                   'r': wordnet.ADV}.get(pos, wordnet.NOUN)\n",
    "            lemmatized_token = lemmatizer.lemmatize(token, pos)\n",
    "        else:\n",
    "            lemmatized_token = tg_stemmer([token])[0]\n",
    "        \n",
    "        lemmatized_tokens.append(lemmatized_token)\n",
    "        \n",
    "    # Join the tokens back into a string\n",
    "    text = ' '.join(lemmatized_tokens)\n",
    "    return text\n",
    "\n",
    "\n",
    "# Load the trained SVM model\n",
    "clf = joblib.load('svm_model.joblib')\n",
    "\n",
    "# Load the vectorizer fitted on the training data\n",
    "vectorizer = joblib.load('tfidf_vectorizer.joblib')\n",
    "\n",
    "# Get input from user\n",
    "sentiment = input(\"Enter a sentence to analyze: \")\n",
    "\n",
    "# Preprocess the input text\n",
    "sentiment_processed = preprocess_text(sentiment)\n",
    "\n",
    "# Vectorize the input text\n",
    "sentiment_vectorized = vectorizer.transform([sentiment_processed])\n",
    "\n",
    "# Predict the sentiment using the trained SVM model\n",
    "prediction = clf.predict(sentiment_vectorized)\n",
    "\n",
    "print(sentiment)\n",
    "# Print the prediction\n",
    "if prediction == 1:\n",
    "    print(\"Negative Statement\")\n",
    "else:\n",
    "    print(\"Positive sentiment\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
